"""
ç®€åŒ–ç‰ˆæ¨¡å‹ç®¡ç†å™¨
åŸºäº LiteLLM çš„è®¾è®¡æ€è·¯ï¼Œå®ç°æ¨¡å‹çš„æ³¨å†Œã€è¯†åˆ«å’Œç®¡ç†åŠŸèƒ½
æ”¯æŒç»Ÿä¸€çš„ API è°ƒç”¨æ¥å£ (model, messages, tools, response_format)
"""

import os
import json
import time
from pathlib import Path
from typing import Optional, Dict, List, Tuple, Any, Union
from dataclasses import dataclass, field, asdict
from dotenv import load_dotenv

from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_google_genai import ChatGoogleGenerativeAI


# åŠ è½½ .env æ–‡ä»¶
load_dotenv()


@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®ç±»"""
    model_name: str
    provider: str
    api_base: Optional[str] = None
    api_key: Optional[str] = None
    default_params: Dict = field(default_factory=dict)
    supports_streaming: bool = True
    supports_functions: bool = False
    supports_vision: bool = False
    max_tokens: Optional[int] = None
    
    # æ–°å¢å­—æ®µï¼Œç”¨äºæŒ‡ç¤ºæ˜¯å¦ä½¿ç”¨ç‰¹æ®Šçš„ responses API (å¦‚ gpt-5)
    use_responses_api: bool = False
    output_version: Optional[str] = None


class TokenUsageCallbackHandler(BaseCallbackHandler):
    """Token ä½¿ç”¨ç»Ÿè®¡å›è°ƒå¤„ç†ç¨‹åº"""
    def __init__(self, model_name: str = "unknown"):
        self.model_name = model_name
        self.input_tokens = 0
        self.output_tokens = 0
        self.total_tokens = 0
        self.total_cost = 0.0
        self.start_time = None
        self.end_time = None
        self.total_duration = 0.0
        self.call_count = 0

    def on_llm_start(self, serialized, prompts, **kwargs):
        """Called when LLM starts running."""
        self.start_time = time.time()

    def on_llm_end(self, response, **kwargs):
        """Called when LLM ends running."""
        if self.start_time is not None:
            self.end_time = time.time()
            duration = self.end_time - self.start_time
            self.total_duration += duration
            self.call_count += 1
        
        usage = None
        
        # Handle LLMResult
        if hasattr(response, "llm_output") and response.llm_output:
            if "token_usage" in response.llm_output:
                usage = response.llm_output["token_usage"]
        
        # Handle direct usage_metadata
        elif hasattr(response, "usage_metadata"):
            usage = response.usage_metadata
            
        if usage:
            input_tokens = usage.get("prompt_tokens", usage.get("input_tokens", 0))
            output_tokens = usage.get("completion_tokens", usage.get("output_tokens", 0))
            total_tokens = usage.get("total_tokens", 0)
            
            self.input_tokens += input_tokens
            self.output_tokens += output_tokens
            self.total_tokens += total_tokens
            
            # print(f"| Model: {self.model_name} | Tokens: {input_tokens} in, {output_tokens} out | Time: {duration:.2f}s")


class ModelManager:
    """
    æ¨¡å‹ç®¡ç†å™¨ - ç®¡ç†å’Œè¯†åˆ«ä¸åŒçš„ LLM æ¨¡å‹
    
    åŠŸèƒ½ï¼š
    1. æ³¨å†Œå’Œç®¡ç†æ¨¡å‹é…ç½®
    2. ç»Ÿä¸€çš„ API è°ƒç”¨æ¥å£ (chat)
    3. è‡ªåŠ¨å®ä¾‹åŒ–å¯¹åº”çš„ LangChain å®¢æˆ·ç«¯
    """
    
    def __init__(self, model_file: str = "model.json"):
        """
        åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨
        
        Args:
            model_file: æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸º model.json
        """
        self.model_file = Path(__file__).parent / model_file
        self.models: Dict[str, ModelConfig] = {}
        self.model_aliases: Dict[str, str] = {}
        
        # ç¼“å­˜å·²å®ä¾‹åŒ–çš„æ¨¡å‹å®¢æˆ·ç«¯
        self._client_cache: Dict[str, Any] = {}
        
        # æä¾›å•†åˆ—è¡¨
        self.providers: List[str] = [
            "openai", "anthropic", "google", "deepseek", "ollama"
        ]
        
        # æä¾›å•†é»˜è®¤ API Base URL æ˜ å°„
        self.provider_api_bases: Dict[str, str] = {
            "openai": "https://api.openai.com/v1",
            "anthropic": "https://api.anthropic.com",
            "google": "https://generativelanguage.googleapis.com/v1",
            "deepseek": "https://api.deepseek.com/v1",
            "ollama": "http://localhost:11434/v1",
        }
        
        # å·²çŸ¥æ¨¡å‹é…ç½®
        self.known_models: Dict[str, str] = {
            "gpt-4o": "openai",
            "gpt-4-turbo": "openai",
            "claude-3-5-sonnet-20241022": "anthropic",
            "gemini-1.5-pro": "google",
            "deepseek-chat": "deepseek"
        }
        
        self._initialize_default_models()
        self._load_from_json()
    
    def _initialize_default_models(self):
        """åˆå§‹åŒ–é»˜è®¤æ”¯æŒçš„æ¨¡å‹é…ç½®"""
        for model_name, provider in self.known_models.items():
            config = ModelConfig(
                model_name=model_name,
                provider=provider,
                api_base=self.provider_api_bases.get(provider),
                supports_vision="gpt-4" in model_name or "claude" in model_name or "gemini" in model_name
            )
            self.models[model_name] = config
    
    def _load_from_json(self):
        """ä» JSON æ–‡ä»¶åŠ è½½è‡ªå®šä¹‰æ¨¡å‹é…ç½®"""
        try:
            if self.model_file.exists():
                with open(self.model_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                if 'custom_models' in data:
                    for model_data in data['custom_models']:
                        config = ModelConfig(
                            model_name=model_data['model_name'],
                            provider=model_data['provider'],
                            api_base=model_data.get('api_base'),
                            api_key=model_data.get('api_key'),
                            default_params=model_data.get('default_params', {}),
                            supports_streaming=model_data.get('supports_streaming', True),
                            supports_functions=model_data.get('supports_functions', False),
                            supports_vision=model_data.get('supports_vision', False),
                            max_tokens=model_data.get('max_tokens'),
                            use_responses_api=model_data.get('use_responses_api', False),
                            output_version=model_data.get('output_version')
                        )
                        self.models[model_data['model_name']] = config
                
                if 'aliases' in data:
                    self.model_aliases = data['aliases']
                    
        except Exception as e:
            print(f"âš ï¸  åŠ è½½æ¨¡å‹é…ç½®å¤±è´¥: {e}")
    
    def _save_to_json(self):
        """ä¿å­˜è‡ªå®šä¹‰æ¨¡å‹é…ç½®åˆ° JSON æ–‡ä»¶"""
        try:
            custom_models = []
            for model_name, config in self.models.items():
                if model_name not in self.known_models:
                    model_data = asdict(config)
                    custom_models.append(model_data)
            
            data = {
                'custom_models': custom_models,
                'aliases': self.model_aliases
            }
            
            with open(self.model_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âŒ ä¿å­˜æ¨¡å‹é…ç½®å¤±è´¥: {e}")

    def register_model(self, model_name: str, provider: str, **kwargs):
        """æ³¨å†Œæ–°æ¨¡å‹"""
        # æå– api_baseï¼Œé¿å…é‡å¤ä¼ é€’
        api_base = kwargs.pop('api_base', None) or self.provider_api_bases.get(provider)
        
        config = ModelConfig(
            model_name=model_name,
            provider=provider,
            api_base=api_base,
            **kwargs
        )
        self.models[model_name] = config
        self._save_to_json()
        # æ¸…é™¤ç¼“å­˜
        if model_name in self._client_cache:
            del self._client_cache[model_name]
        return config

    def get_model_config(self, model: str) -> Optional[ModelConfig]:
        """è·å–æ¨¡å‹é…ç½®"""
        if model in self.model_aliases:
            model = self.model_aliases[model]
        return self.models.get(model)

    def _get_api_key(self, provider: str, config_key: Optional[str] = None) -> Optional[str]:
        """è·å– API Key (ä¼˜å…ˆä½¿ç”¨ç»Ÿä¸€çš„ API_KEY)"""
        # 1. ä¼˜å…ˆæ£€æŸ¥ç¯å¢ƒå˜é‡ä¸­çš„ç»Ÿä¸€ API_KEY
        unified_key = os.getenv("API_KEY")
        if unified_key:
            return unified_key
            
        # 2. å¦‚æœé…ç½®ä¸­æœ‰æ˜¾å¼çš„ keyï¼Œä½¿ç”¨å®ƒ
        if config_key:
            return config_key
        
        # 3. æ£€æŸ¥æä¾›å•†ç‰¹å®šçš„ç¯å¢ƒå˜é‡
        env_keys = {
            "openai": "OPENAI_API_KEY",
            "anthropic": "ANTHROPIC_API_KEY",
            "google": "GOOGLE_API_KEY",
            "deepseek": "DEEPSEEK_API_KEY",
        }
        return os.getenv(env_keys.get(provider, ""))

    def _get_api_base(self, provider: str, config_base: Optional[str] = None) -> Optional[str]:
        """è·å– API Base (ä¼˜å…ˆä½¿ç”¨ç»Ÿä¸€çš„ BASE_URL)"""
        # 1. ä¼˜å…ˆæ£€æŸ¥ç¯å¢ƒå˜é‡ä¸­çš„ç»Ÿä¸€ BASE_URL
        unified_base = os.getenv("BASE_URL")
        if unified_base:
            return unified_base
            
        # 2. æ£€æŸ¥æä¾›å•†ç‰¹å®šçš„ç¯å¢ƒå˜é‡
        env_var = f"{provider.upper()}_API_BASE"
        env_base = os.getenv(env_var)
        if env_base:
            return env_base

        # 3. ä½¿ç”¨é…ç½®ä¸­çš„ base (æˆ–è€…é»˜è®¤å€¼)
        return config_base

    def _create_client(self, config: ModelConfig):
        """åˆ›å»º LangChain å®¢æˆ·ç«¯å®ä¾‹"""
        api_key = self._get_api_key(config.provider, config.api_key)
        api_base = self._get_api_base(config.provider, config.api_base)
        
        if not api_key and config.provider != "ollama":
             print(f"âš ï¸  Warning: No API key found for {config.provider}")

        callbacks = [TokenUsageCallbackHandler(config.model_name)]
        
        common_args = {
            "model": config.model_name,
            "api_key": api_key,
            "base_url": api_base,
            "callbacks": callbacks,
            "max_tokens": config.max_tokens,
            **config.default_params
        }
        
        # ç§»é™¤ None å€¼å‚æ•°
        common_args = {k: v for k, v in common_args.items() if v is not None}

        if config.provider == "openai" or config.provider == "deepseek":
            # DeepSeek å…¼å®¹ OpenAI æ¥å£
            if config.use_responses_api:
                common_args["use_responses_api"] = True
                if config.output_version:
                    common_args["output_version"] = config.output_version
            return ChatOpenAI(**common_args)
            
        elif config.provider == "anthropic":
            return ChatAnthropic(**common_args)
            
        elif config.provider == "google":
            # Google GenAI å‚æ•°ç¨æœ‰ä¸åŒ
            if "base_url" in common_args:
                del common_args["base_url"] # Google usually doesn't use base_url this way in LangChain
            return ChatGoogleGenerativeAI(**common_args)
            
        elif config.provider == "ollama":
            # Ollama use ChatOpenAI compatible endpoint usually
            return ChatOpenAI(**common_args)
            
        else:
            # é»˜è®¤å°è¯•ç”¨ ChatOpenAI å…¼å®¹æ¨¡å¼
            return ChatOpenAI(**common_args)

    def get_model(self, model_name: str):
        """è·å–æ¨¡å‹å®ä¾‹ (å¸¦ç¼“å­˜)"""
        if model_name in self.model_aliases:
            model_name = self.model_aliases[model_name]
            
        if model_name in self._client_cache:
            return self._client_cache[model_name]
            
        config = self.get_model_config(model_name)
        if not config:
            # å°è¯•ä½œä¸º OpenAI å…¼å®¹æ¨¡å‹ç›´æ¥åˆ›å»º
            config = ModelConfig(model_name=model_name, provider="openai")
            
        client = self._create_client(config)
        self._client_cache[model_name] = client
        return client

    def chat(
        self, 
        model: str, 
        messages: List[Any], 
        tools: Optional[List[Dict]] = None,
        response_format: Optional[Dict] = None,
        stream: bool = False,
        use_responses_api: Optional[bool] = None,
        **kwargs
    ) -> Dict:
        """
        ç»Ÿä¸€ API è°ƒç”¨æ¥å£ - è¿”å›åŸå§‹ JSON æ ¼å¼å“åº”
        
        Args:
            model: æ¨¡å‹åç§°
            messages: å·²æ ¼å¼åŒ–çš„æ¶ˆæ¯åˆ—è¡¨ (LangChain Message å¯¹è±¡)
            tools: å·¥å…·å®šä¹‰åˆ—è¡¨
            response_format: å“åº”æ ¼å¼å®šä¹‰
            stream: æ˜¯å¦æµå¼è¾“å‡º
            use_responses_api: æ˜¯å¦ä½¿ç”¨ responses APIï¼ˆNone æ—¶ä½¿ç”¨é…ç½®æ–‡ä»¶è®¾ç½®ï¼‰
            **kwargs: å…¶ä»–å‚æ•°
        
        Returns:
            Dict: åŸå§‹ OpenAI API æ ¼å¼çš„ JSON å“åº”
            æ ¼å¼: {
                "id": "chatcmpl-xxx",
                "model": "gpt-4o",
                "choices": [{"message": {"content": "...", "role": "assistant"}}],
                "usage": {"prompt_tokens": 10, "completion_tokens": 20, "total_tokens": 30}
            }
        """
        # è·å–æ¨¡å‹é…ç½®
        config = self.get_model_config(model) or ModelConfig(model, "openai")
        api_key = self._get_api_key(config.provider, config.api_key)
        api_base = self._get_api_base(config.provider, config.api_base)
        
        # ç¡®å®šä½¿ç”¨å“ªç§ APIï¼šä¼˜å…ˆä½¿ç”¨å‚æ•°ï¼Œå…¶æ¬¡ä½¿ç”¨é…ç½®
        if use_responses_api is None:
            use_responses_api = config.use_responses_api
        
        # è½¬æ¢ LangChain Messages ä¸º API æ ¼å¼
        from message_manager import MessageManager
        msg_manager = MessageManager(
            api_type="responses" if use_responses_api else "chat/completions",
            model=model
        )
        api_messages = msg_manager(messages)
        
        # æ„å»ºè¯·æ±‚å‚æ•°
        import requests
        
        # æ ¹æ® API ç±»å‹é€‰æ‹©ç«¯ç‚¹
        if use_responses_api:
            endpoint = f"{api_base.rstrip('/')}/responses"
        else:
            endpoint = f"{api_base.rstrip('/')}/chat/completions"
        
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": model,
            "messages": api_messages,
            "stream": stream,
            **kwargs
        }
        
        # æ·»åŠ å¯é€‰å‚æ•°
        if tools:
            payload["tools"] = tools
        if response_format:
            payload["response_format"] = response_format
        if config.max_tokens:
            payload["max_tokens"] = config.max_tokens
        
        # å‘é€è¯·æ±‚
        response = requests.post(endpoint, json=payload, headers=headers)
        response.raise_for_status()
        
        return response.json()

# å…¨å±€å®ä¾‹
model_manager = ModelManager()


def completion(
    model: str,
    messages: List[Any],
    tools: Optional[List[Dict]] = None,
    response_format: Optional[Dict] = None,
    stream: bool = False,
    **kwargs
) -> Dict:
    """
    Completion API è°ƒç”¨ (æ ‡å‡† chat/completions æ¥å£)
    
    é€‚ç”¨äºå¤§å¤šæ•°æ¨¡å‹ï¼šGPT-4, Claude, Gemini, DeepSeek ç­‰
    è¿”å›åŸå§‹ OpenAI API JSON æ ¼å¼å“åº”
    è‡ªåŠ¨ä½¿ç”¨ chat/completions API ç«¯ç‚¹
    
    Args:
        model: æ¨¡å‹åç§°ï¼Œæ ¼å¼ä¸º "provider/model" æˆ– "model"
               ä¾‹å¦‚: "openai/gpt-4o", "gpt-4o", "anthropic/claude-3-5-sonnet-20241022"
        messages: æ¶ˆæ¯åˆ—è¡¨ (LangChain Message å¯¹è±¡)
        tools: å·¥å…·å®šä¹‰åˆ—è¡¨
        response_format: å“åº”æ ¼å¼å®šä¹‰
        stream: æ˜¯å¦æµå¼è¾“å‡º
        **kwargs: å…¶ä»–å‚æ•°
    
    Returns:
        Dict: åŸå§‹ JSON æ ¼å¼å“åº”
        {
            "id": "chatcmpl-xxx",
            "model": "gpt-4o",
            "choices": [{"message": {"content": "...", "role": "assistant"}}],
            "usage": {"prompt_tokens": 10, "completion_tokens": 20, "total_tokens": 30}
        }
    
    Example:
        >>> from langchain_core.messages import HumanMessage
        >>> response = completion(
        ...     model="openai/gpt-4o",
        ...     messages=[HumanMessage(content="Hello!")]
        ... )
        >>> print(response['choices'][0]['message']['content'])
        >>> print(response['usage'])
    """
    # è§£ææ¨¡å‹åç§° (æ”¯æŒ provider/model æ ¼å¼)
    if "/" in model:
        provider, model_name = model.split("/", 1)
        model = model_name
    
    # è‡ªåŠ¨è®¾ç½®ä½¿ç”¨ chat/completions API
    return model_manager.chat(
        model=model,
        messages=messages,
        tools=tools,
        response_format=response_format,
        stream=stream,
        use_responses_api=False,  # completion() å¼ºåˆ¶ä½¿ç”¨ chat/completions
        **kwargs
    )


def response(
    model: str,
    messages: List[Any],
    tools: Optional[List[Dict]] = None,
    response_format: Optional[Dict] = None,
    stream: bool = False,
    **kwargs
) -> Dict:
    """
    Response API è°ƒç”¨ (æ–°ç‰ˆ responses æ¥å£ï¼Œå¦‚ GPT-5)
    
    é€‚ç”¨äºä½¿ç”¨ responses API çš„æ¨¡å‹ï¼ˆå¦‚ gpt-5ï¼‰
    è¿”å›åŸå§‹ JSON æ ¼å¼å“åº”
    è‡ªåŠ¨ä½¿ç”¨ responses API ç«¯ç‚¹
    
    Args:
        model: æ¨¡å‹åç§°ï¼Œæ ¼å¼ä¸º "provider/model" æˆ– "model"
               ä¾‹å¦‚: "openai/gpt-5", "gpt-5"
        messages: æ¶ˆæ¯åˆ—è¡¨ (LangChain Message å¯¹è±¡)
        tools: å·¥å…·å®šä¹‰åˆ—è¡¨
        response_format: å“åº”æ ¼å¼å®šä¹‰
        stream: æ˜¯å¦æµå¼è¾“å‡º
        **kwargs: å…¶ä»–å‚æ•°
    
    Returns:
        Dict: åŸå§‹ JSON æ ¼å¼å“åº”
    
    Example:
        >>> from langchain_core.messages import HumanMessage
        >>> resp = response(
        ...     model="openai/gpt-5",
        ...     messages=[HumanMessage(content="Hello!")]
        ... )
        >>> print(resp['choices'][0]['message']['content'])
    """
    # è§£ææ¨¡å‹åç§°
    if "/" in model:
        provider, model_name = model.split("/", 1)
        model = model_name
    
    # è‡ªåŠ¨è®¾ç½®ä½¿ç”¨ responses API
    return model_manager.chat(
        model=model,
        messages=messages,
        tools=tools,
        response_format=response_format,
        stream=stream,
        use_responses_api=True,  # response() å¼ºåˆ¶ä½¿ç”¨ responses API
        **kwargs
    )


if __name__ == "__main__":
    print("ğŸš€ æ¨¡å‹ç®¡ç†å™¨æµ‹è¯•")
    
    # ç®€å•çš„æµ‹è¯• (å¦‚æœç¯å¢ƒä¸­æœ‰ key)
    try:
        from langchain_core.messages import HumanMessage
        import json
        
        print("\n1ï¸âƒ£ æµ‹è¯• completion API...")
        messages = [HumanMessage(content="Say hello in one word")]
        resp = completion(model="openai/gpt-4o", messages=messages)
        print(f"Response type: {type(resp).__name__}")
        print(f"Response: {json.dumps(resp, indent=2, ensure_ascii=False)}")
        print(f"Content: {resp['choices'][0]['message']['content']}")
        print(f"Usage: {resp['usage']}")
        
        print("\n2ï¸âƒ£ æµ‹è¯• response API...")
        # æ³¨æ„ï¼šéœ€è¦æ¨¡å‹æ”¯æŒ responses API
        # resp = response(model="openai/gpt-5", messages=messages)
        # print(f"Content: {resp['choices'][0]['message']['content']}")
        
    except Exception as e:
        print(f"Test failed: {e}")
