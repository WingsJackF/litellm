"""
ç®€åŒ–ç‰ˆæ¨¡å‹ç®¡ç†å™¨
åŸºäº LiteLLM çš„è®¾è®¡æ€è·¯ï¼Œå®ç°æ¨¡å‹çš„æ³¨å†Œã€è¯†åˆ«å’Œç®¡ç†åŠŸèƒ½
æ”¯æŒç»Ÿä¸€çš„ API è°ƒç”¨æ¥å£ (model, messages, tools, response_format)
"""

import os
import json
from pathlib import Path
from typing import Optional, Dict, List, Any, Union
from dataclasses import dataclass, field, asdict
from dotenv import load_dotenv
from openai import OpenAI

# åŠ è½½ .env æ–‡ä»¶ï¼ˆå§‹ç»ˆä»å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•åŠ è½½ï¼‰
_env_path = Path(__file__).parent / ".env"
load_dotenv(_env_path, override=True)  # override=True ç¡®ä¿ .env ä¼˜å…ˆäºç³»ç»Ÿç¯å¢ƒå˜é‡


@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®ç±»"""
    model_name: str
    provider: str
    api_base: Optional[str] = None
    api_key: Optional[str] = None
    default_params: Dict = field(default_factory=dict)
    supports_streaming: bool = True
    supports_functions: bool = False
    supports_vision: bool = False
    max_tokens: Optional[int] = None
    
    # æ–°å¢å­—æ®µï¼Œç”¨äºæŒ‡ç¤ºæ˜¯å¦ä½¿ç”¨ç‰¹æ®Šçš„ responses API (å¦‚ gpt-5)
    use_responses_api: bool = False
    output_version: Optional[str] = None


@dataclass
class LLMResponse:
    """
    LLM å“åº”åŒ…è£…ç±»
    
    æ ¹æ® response_type è¿”å›ä¸åŒæ ¼å¼çš„æ•°æ®ï¼š
    - "content": åªè¿”å›å“åº”å†…å®¹ (å­—ç¬¦ä¸²)
    - "raw": è¿”å›åŸå§‹ API å“åº” (å®Œæ•´ dict)
    
    æ”¯æŒä¸¤ç§ API å“åº”æ ¼å¼ï¼š
    - chat/completions: choices[0].message.content
    - responses API: output[0].content[0].text
    
    Example:
        >>> resp = LLMResponse(raw_response=api_result, response_type="content")
        >>> print(resp.get())  # åªè¿”å›å†…å®¹å­—ç¬¦ä¸²
        
        >>> resp = LLMResponse(raw_response=api_result, response_type="raw")
        >>> print(resp.get())  # è¿”å›å®Œæ•´çš„åŸå§‹å“åº”
    """
    raw_response: Dict[str, Any]
    response_type: str = "content"  # "content" æˆ– "raw"
    
    @property
    def content(self) -> str:
        """è·å–å“åº”å†…å®¹ï¼ˆè‡ªåŠ¨é€‚é…ä¸åŒ API æ ¼å¼ï¼‰"""
        if self.raw_response is None:
            return ""
        
        try:
            # æ ¼å¼1: æ ‡å‡† chat/completions API
            # {"choices": [{"message": {"content": "..."}}]}
            if 'choices' in self.raw_response:
                return self.raw_response['choices'][0]['message']['content'] or ""
            
            # æ ¼å¼2: OpenAI responses API (å¦‚ o1, o3, o4-mini ç­‰)
            # {"output": [{"type": "message", "content": [{"type": "output_text", "text": "..."}]}]}
            if 'output' in self.raw_response:
                for item in self.raw_response['output']:
                    if item.get('type') == 'message':
                        for content_item in item.get('content', []):
                            if content_item.get('type') == 'output_text':
                                return content_item.get('text', '')
            
            return ""
        except (KeyError, IndexError, TypeError):
            return ""
    
    @property
    def raw(self) -> Dict[str, Any]:
        """è·å–åŸå§‹å“åº”"""
        return self.raw_response
    
    @property
    def usage(self) -> Optional[Dict[str, int]]:
        """è·å– token ä½¿ç”¨æƒ…å†µ"""
        if self.raw_response is None:
            return None
        return self.raw_response.get('usage')
    
    @property
    def model(self) -> Optional[str]:
        """è·å–å®é™…ä½¿ç”¨çš„æ¨¡å‹åç§°"""
        if self.raw_response is None:
            return None
        return self.raw_response.get('model')
    
    def get(self) -> Union[str, Dict[str, Any]]:
        """æ ¹æ® response_type è¿”å›å¯¹åº”æ•°æ®"""
        if self.response_type == "raw":
            return self.raw_response
        return self.content
    
    def __str__(self) -> str:
        """å­—ç¬¦ä¸²è¡¨ç¤ºï¼Œè¿”å›å†…å®¹"""
        return self.content
    
    def __repr__(self) -> str:
        content_preview = self.content[:50] if self.content else ""
        return f"LLMResponse(response_type='{self.response_type}', content='{content_preview}...')"


class ModelManager:
    """
    æ¨¡å‹ç®¡ç†å™¨ - ç®¡ç†å’Œè¯†åˆ«ä¸åŒçš„ LLM æ¨¡å‹
    
    åŠŸèƒ½ï¼š
    1. æ³¨å†Œå’Œç®¡ç†æ¨¡å‹é…ç½®
    2. ç»Ÿä¸€çš„ API è°ƒç”¨æ¥å£ (chat)
    """
    
    def __init__(self, model_file: str = "model.json"):
        """
        åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨
        
        Args:
            model_file: æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸º model.json
        """
        self.model_file = Path(__file__).parent / model_file
        self.models: Dict[str, ModelConfig] = {}
        self.model_aliases: Dict[str, str] = {}
        
        # æä¾›å•†åˆ—è¡¨
        self.providers: List[str] = [
            "openai", "anthropic", "google", "deepseek"
        ]
        
        # æä¾›å•†é»˜è®¤ API Base URL æ˜ å°„
        self.provider_api_bases: Dict[str, str] = {
            "openai": "https://api.openai.com/v1",
            "anthropic": "https://api.anthropic.com",
            "google": "https://generativelanguage.googleapis.com/v1",
            "deepseek": "https://api.deepseek.com/v1",
            "ollama": "http://localhost:11434/v1",
        }
        
        # å·²çŸ¥æ¨¡å‹é…ç½®
        self.known_models: Dict[str, str] = {
            "gpt-4o": "openai",
            "gpt-4-turbo": "openai",
            "gpt-5": "openai",
            "o3": "openai",
            "o3-deep-research": "openai",  # Deep Research æ¨¡å‹
            "computer-use-preview": "openai",  # Computer Use æ¨¡å‹
            "claude-3-5-sonnet-20241022": "anthropic",
            "gemini-1.5-pro": "google",
            "deepseek-chat": "deepseek"
        }
        
        self._initialize_default_models()
        self._load_from_json()
    
    def _initialize_default_models(self):
        """åˆå§‹åŒ–é»˜è®¤æ”¯æŒçš„æ¨¡å‹é…ç½®"""
        for model_name, provider in self.known_models.items():
            config = ModelConfig(
                model_name=model_name,
                provider=provider,
                api_base=self.provider_api_bases.get(provider),
                supports_vision="gpt-4" in model_name or "claude" in model_name or "gemini" in model_name
            )
            self.models[model_name] = config
    
    def _load_from_json(self):
        """ä» JSON æ–‡ä»¶åŠ è½½è‡ªå®šä¹‰æ¨¡å‹é…ç½®"""
        try:
            if self.model_file.exists():
                with open(self.model_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                if 'custom_models' in data:
                    for model_data in data['custom_models']:
                        config = ModelConfig(
                            model_name=model_data['model_name'],
                            provider=model_data['provider'],
                            api_base=model_data.get('api_base'),
                            api_key=model_data.get('api_key'),
                            default_params=model_data.get('default_params', {}),
                            supports_streaming=model_data.get('supports_streaming', True),
                            supports_functions=model_data.get('supports_functions', False),
                            supports_vision=model_data.get('supports_vision', False),
                            max_tokens=model_data.get('max_tokens'),
                            use_responses_api=model_data.get('use_responses_api', False),
                            output_version=model_data.get('output_version')
                        )
                        self.models[model_data['model_name']] = config
                
                if 'aliases' in data:
                    self.model_aliases = data['aliases']
                    
        except Exception as e:
            print(f"âš ï¸  åŠ è½½æ¨¡å‹é…ç½®å¤±è´¥: {e}")
    
    def _save_to_json(self):
        """ä¿å­˜è‡ªå®šä¹‰æ¨¡å‹é…ç½®åˆ° JSON æ–‡ä»¶"""
        try:
            custom_models = []
            for model_name, config in self.models.items():
                if model_name not in self.known_models:
                    model_data = asdict(config)
                    custom_models.append(model_data)
            
            data = {
                'custom_models': custom_models,
                'aliases': self.model_aliases
            }
            
            with open(self.model_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âŒ ä¿å­˜æ¨¡å‹é…ç½®å¤±è´¥: {e}")

    def register_model(self, model_name: str, provider: str, **kwargs):
        """æ³¨å†Œæ–°æ¨¡å‹"""
        # æå– api_baseï¼Œé¿å…é‡å¤ä¼ é€’
        api_base = kwargs.pop('api_base', None) or self.provider_api_bases.get(provider)
        
        config = ModelConfig(
            model_name=model_name,
            provider=provider,
            api_base=api_base,
            **kwargs
        )
        self.models[model_name] = config
        self._save_to_json()
        return config

    def get_model_config(self, model: str) -> Optional[ModelConfig]:
        """è·å–æ¨¡å‹é…ç½®"""
        if model in self.model_aliases:
            model = self.model_aliases[model]
        return self.models.get(model)

    def _get_api_key(self, provider: str, config_key: Optional[str] = None, use_provider_specific: bool = False) -> Optional[str]:
        """
        è·å– API Key
        
        Args:
            provider: æä¾›å•†åç§°
            config_key: é…ç½®ä¸­çš„ key
            use_provider_specific: æ˜¯å¦ä¼˜å…ˆä½¿ç”¨æä¾›å•†ç‰¹å®šçš„ç¯å¢ƒå˜é‡ï¼ˆç”¨äº responses APIï¼‰
        """
        # æä¾›å•†ç‰¹å®šçš„ç¯å¢ƒå˜é‡æ˜ å°„
        env_keys = {
            "openai": "OPENAI_API_KEY",
            "anthropic": "ANTHROPIC_API_KEY",
            "google": "GOOGLE_API_KEY",
            "deepseek": "DEEPSEEK_API_KEY",
        }
        
        if use_provider_specific:
            # responses API: ä¼˜å…ˆä½¿ç”¨æä¾›å•†ç‰¹å®šçš„ç¯å¢ƒå˜é‡
            # 1. æä¾›å•†ç‰¹å®šçš„ç¯å¢ƒå˜é‡
            provider_key = os.getenv(env_keys.get(provider, ""))
            if provider_key:
                return provider_key
            # 2. é…ç½®ä¸­çš„ key
            if config_key:
                return config_key
            # 3. ç»Ÿä¸€çš„ API_KEY ä½œä¸ºåå¤‡
            return os.getenv("API_KEY")
        else:
            # completion API: ä¼˜å…ˆä½¿ç”¨ç»Ÿä¸€çš„ API_KEYï¼ˆä»£ç†ï¼‰
            # 1. ç»Ÿä¸€çš„ API_KEY
            unified_key = os.getenv("API_KEY")
            if unified_key:
                return unified_key
            # 2. é…ç½®ä¸­çš„ key
            if config_key:
                return config_key
            # 3. æä¾›å•†ç‰¹å®šçš„ç¯å¢ƒå˜é‡
            return os.getenv(env_keys.get(provider, ""))

    def _get_api_base(self, provider: str, config_base: Optional[str] = None, use_provider_specific: bool = False) -> Optional[str]:
        """
        è·å– API Base
        
        Args:
            provider: æä¾›å•†åç§°
            config_base: é…ç½®ä¸­çš„ base
            use_provider_specific: æ˜¯å¦ä¼˜å…ˆä½¿ç”¨æä¾›å•†ç‰¹å®šçš„ç¯å¢ƒå˜é‡ï¼ˆç”¨äº responses APIï¼‰
        """
        # æä¾›å•†ç‰¹å®šçš„ç¯å¢ƒå˜é‡
        env_var = f"{provider.upper()}_API_BASE"
        
        if use_provider_specific:
            # responses API: ä¼˜å…ˆä½¿ç”¨æä¾›å•†ç‰¹å®šçš„ç¯å¢ƒå˜é‡
            # 1. æä¾›å•†ç‰¹å®šçš„ç¯å¢ƒå˜é‡
            provider_base = os.getenv(env_var)
            if provider_base:
                return provider_base
            # 2. é…ç½®ä¸­çš„ base
            if config_base:
                return config_base
            # 3. é»˜è®¤çš„æä¾›å•† API base
            return self.provider_api_bases.get(provider)
        else:
            # completion API: ä¼˜å…ˆä½¿ç”¨ç»Ÿä¸€çš„ BASE_URLï¼ˆä»£ç†ï¼‰
            # 1. ç»Ÿä¸€çš„ BASE_URL
            unified_base = os.getenv("BASE_URL")
            if unified_base:
                return unified_base
            # 2. æä¾›å•†ç‰¹å®šçš„ç¯å¢ƒå˜é‡
            env_base = os.getenv(env_var)
            if env_base:
                return env_base
            # 3. é…ç½®ä¸­çš„ base
            return config_base

    def chat(
        self, 
        model: str, 
        messages: List[Any], 
        tools: Optional[List[Dict]] = None,
        response_format: Optional[Dict] = None,
        stream: bool = False,
        use_responses_api: Optional[bool] = None,
        use_provider_api: bool = False,
        provider: Optional[str] = None,
        **kwargs
    ) -> Union[Dict, Any]:
        """
        ç»Ÿä¸€ API è°ƒç”¨æ¥å£ - ä½¿ç”¨ OpenAI SDK
        
        Args:
            model: æ¨¡å‹åç§°
            messages: æ¶ˆæ¯åˆ—è¡¨ (HumanMessage, AIMessage, SystemMessage å¯¹è±¡)
            tools: å·¥å…·å®šä¹‰åˆ—è¡¨
            response_format: å“åº”æ ¼å¼å®šä¹‰
            stream: æ˜¯å¦æµå¼è¾“å‡º
            use_responses_api: æ˜¯å¦ä½¿ç”¨ responses APIï¼ˆNone æ—¶ä½¿ç”¨é…ç½®æ–‡ä»¶è®¾ç½®ï¼‰
            use_provider_api: æ˜¯å¦ä½¿ç”¨å‚å•†åŸå§‹ APIï¼ˆTrue æ—¶ä¼˜å…ˆä½¿ç”¨ PROVIDER_API_KEY/BASEï¼‰
            provider: æä¾›å•†åç§°ï¼ˆå¦‚ openai, anthropic, google ç­‰ï¼‰
            **kwargs: å…¶ä»–å‚æ•°
        
        Returns:
            Dict: åŸå§‹ OpenAI API æ ¼å¼çš„ JSON å“åº”ï¼ˆéæµå¼ï¼‰
            Stream: æµå¼å“åº”å¯¹è±¡ï¼ˆæµå¼ï¼‰
            æ ¼å¼: {
                "id": "chatcmpl-xxx",
                "model": "gpt-4o",
                "choices": [{"message": {"content": "...", "role": "assistant"}}],
                "usage": {"prompt_tokens": 10, "completion_tokens": 20, "total_tokens": 30}
            }
        """
        # è·å–æ¨¡å‹é…ç½®ï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ™ä½¿ç”¨ä¼ å…¥çš„ provider æˆ–é»˜è®¤ openai
        config = self.get_model_config(model)
        if config is None:
            default_provider = provider or "openai"
            config = ModelConfig(model, default_provider)
        
        # ç¡®å®šä½¿ç”¨å“ªç§ APIï¼šä¼˜å…ˆä½¿ç”¨å‚æ•°ï¼Œå…¶æ¬¡ä½¿ç”¨é…ç½®
        if use_responses_api is None:
            use_responses_api = config.use_responses_api
        
        # æ ¹æ® API ç±»å‹å†³å®šè·å– key å’Œ base çš„ä¼˜å…ˆçº§
        # use_provider_api=True: å¼ºåˆ¶ä½¿ç”¨å‚å•†åŸå§‹ API (PROVIDER_API_KEY, PROVIDER_API_BASE)
        # responses API: ä¼˜å…ˆä½¿ç”¨æä¾›å•†ç‰¹å®šçš„ç¯å¢ƒå˜é‡ (OPENAI_API_KEY, OPENAI_API_BASE)
        # completion API: ä¼˜å…ˆä½¿ç”¨ç»Ÿä¸€çš„ä»£ç† (API_KEY, BASE_URL)
        use_provider_specific = use_provider_api or use_responses_api
        api_key = self._get_api_key(config.provider, config.api_key, use_provider_specific=use_provider_specific)
        api_base = self._get_api_base(config.provider, config.api_base, use_provider_specific=use_provider_specific)
        
        # è½¬æ¢ Messages ä¸º API æ ¼å¼
        from message_manager import MessageManager
        msg_manager = MessageManager(
            api_type="responses" if use_responses_api else "chat/completions",
            model=model
        )
        api_messages = msg_manager(messages)
        
        # åˆ›å»º OpenAI å®¢æˆ·ç«¯ï¼ˆæ·»åŠ è¶…æ—¶è®¾ç½®ï¼‰
        timeout = kwargs.pop('timeout', 120)  # é»˜è®¤ 120 ç§’è¶…æ—¶
        client = OpenAI(
            api_key=api_key,
            base_url=api_base,
            timeout=timeout
        )
        
        # æ ¹æ® API ç±»å‹é€‰æ‹©ä¸åŒçš„è°ƒç”¨æ–¹å¼
        if use_responses_api:
            # ä½¿ç”¨ responses APIï¼ˆå¦‚ GPT-5, o1, o3 ç­‰ï¼‰
            # ç«¯ç‚¹: /responses
            params = {
                "model": model,
                "input": api_messages,  # responses API ä½¿ç”¨ input è€Œä¸æ˜¯ messages
                "stream": stream,
                **kwargs
            }
            
            # æ·»åŠ å¯é€‰å‚æ•°
            if tools:
                params["tools"] = tools
            if config.max_tokens:
                params["max_tokens"] = config.max_tokens
            
            # è°ƒè¯•è¾“å‡º
            print(f"ğŸ”„ Calling API: {api_base}/responses")
            print(f"   Model: {model}, Timeout: {timeout}s")
            
            # è°ƒç”¨ responses API
            response = client.responses.create(**params)
        else:
            # ä½¿ç”¨æ ‡å‡† chat/completions API
            params = {
                "model": model,
                "messages": api_messages,
                "stream": stream,
                **kwargs
            }
            
            # æ·»åŠ å¯é€‰å‚æ•°
            if tools:
                params["tools"] = tools
            if response_format:
                params["response_format"] = response_format
            if config.max_tokens:
                params["max_tokens"] = config.max_tokens
            
            # è°ƒè¯•è¾“å‡º
            print(f"ğŸ”„ Calling API: {api_base}/chat/completions")
            print(f"   Model: {model}, Timeout: {timeout}s")
            
            # è°ƒç”¨ chat/completions API
            response = client.chat.completions.create(**params)
        
        # æµå¼å“åº”ç›´æ¥è¿”å›
        if stream:
            return response
        
        # éæµå¼å“åº”è½¬æ¢ä¸º dict
        return response.model_dump()

# å…¨å±€å®ä¾‹
model_manager = ModelManager()


def completion(
    model: str,
    messages: List[Any],
    tools: Optional[List[Dict]] = None,
    response_format: Optional[Dict] = None,
    stream: bool = False,
    response_type: str = "raw",
    use_provider_api: bool = False,
    **kwargs
) -> Union[str, Dict, LLMResponse]:
    """
    Completion API è°ƒç”¨ (æ ‡å‡† chat/completions æ¥å£)
    
    é€‚ç”¨äºå¤§å¤šæ•°æ¨¡å‹ï¼šGPT-4, Claude, Gemini, DeepSeek ç­‰
    è‡ªåŠ¨ä½¿ç”¨ chat/completions API ç«¯ç‚¹
    
    Args:
        model: æ¨¡å‹åç§°ï¼Œæ ¼å¼ä¸º "provider/model" æˆ– "model"
               ä¾‹å¦‚: "openai/gpt-4o", "gpt-4o", "anthropic/claude-3-5-sonnet-20241022"
        messages: æ¶ˆæ¯åˆ—è¡¨ (HumanMessage, AIMessage, SystemMessage å¯¹è±¡)
        tools: å·¥å…·å®šä¹‰åˆ—è¡¨
        response_format: å“åº”æ ¼å¼å®šä¹‰
        stream: æ˜¯å¦æµå¼è¾“å‡º
        response_type: å“åº”ç±»å‹
            - "content": åªè¿”å›å†…å®¹å­—ç¬¦ä¸²
            - "raw": è¿”å›åŸå§‹ API å“åº” dict (é»˜è®¤)
        use_provider_api: æ˜¯å¦ä½¿ç”¨å‚å•†åŸå§‹ APIï¼ˆTrue æ—¶ä½¿ç”¨ PROVIDER_API_KEY/BASEï¼‰
        **kwargs: å…¶ä»–å‚æ•°
    
    Returns:
        æ ¹æ® response_type è¿”å›:
        - "content": str (å“åº”å†…å®¹)
        - "raw": Dict (åŸå§‹ JSON æ ¼å¼å“åº”)
    
    Example:
        >>> from message_manager import HumanMessage
        >>> # è·å–åŸå§‹å“åº”
        >>> resp = completion(model="gpt-4o", messages=[HumanMessage(content="Hello!")])
        >>> print(resp['choices'][0]['message']['content'])
        
        >>> # åªè·å–å†…å®¹
        >>> content = completion(model="gpt-4o", messages=[HumanMessage(content="Hello!")], response_type="content")
        >>> print(content)  # ç›´æ¥è¾“å‡ºå­—ç¬¦ä¸²
        
        >>> # ä½¿ç”¨å‚å•†åŸå§‹ APIï¼ˆComputer Use ç­‰åœºæ™¯ï¼‰
        >>> resp = completion(model="anthropic/claude", messages=msgs, use_provider_api=True)
    """
    # è§£ææ¨¡å‹åç§° (æ”¯æŒ provider/model æ ¼å¼)
    provider = None
    if "/" in model:
        provider, model_name = model.split("/", 1)
        model = model_name
    
    # è‡ªåŠ¨è®¾ç½®ä½¿ç”¨ chat/completions API
    raw_response = model_manager.chat(
        model=model,
        messages=messages,
        tools=tools,
        response_format=response_format,
        stream=stream,
        use_responses_api=False,  # completion() å¼ºåˆ¶ä½¿ç”¨ chat/completions
        use_provider_api=use_provider_api,  # æ˜¯å¦ä½¿ç”¨å‚å•†åŸå§‹ API
        provider=provider,  # ä¼ é€’è§£æå‡ºçš„ provider
        **kwargs
    )
    
    # æµå¼å“åº”ç›´æ¥è¿”å›
    if stream:
        return raw_response
    
    # æ ¹æ® response_type è¿”å›å¯¹åº”æ ¼å¼
    llm_response = LLMResponse(raw_response=raw_response, response_type=response_type)
    return llm_response.get()


def response(
    model: str,
    messages: List[Any],
    tools: Optional[List[Dict]] = None,
    response_format: Optional[Dict] = None,
    stream: bool = False,
    response_type: str = "raw",
    use_provider_api: bool = False,
    **kwargs
) -> Union[str, Dict, LLMResponse]:
    """
    Response API è°ƒç”¨ (æ–°ç‰ˆ responses æ¥å£ï¼Œå¦‚ GPT-5)
    
    é€‚ç”¨äºä½¿ç”¨ responses API çš„æ¨¡å‹ï¼ˆå¦‚ gpt-5ï¼‰
    è‡ªåŠ¨ä½¿ç”¨ responses API ç«¯ç‚¹
    
    Args:
        model: æ¨¡å‹åç§°ï¼Œæ ¼å¼ä¸º "provider/model" æˆ– "model"
               ä¾‹å¦‚: "openai/gpt-5", "gpt-5"
        messages: æ¶ˆæ¯åˆ—è¡¨ (HumanMessage, AIMessage, SystemMessage å¯¹è±¡)
        tools: å·¥å…·å®šä¹‰åˆ—è¡¨
        response_format: å“åº”æ ¼å¼å®šä¹‰
        stream: æ˜¯å¦æµå¼è¾“å‡º
        response_type: å“åº”ç±»å‹
            - "content": åªè¿”å›å†…å®¹å­—ç¬¦ä¸²
            - "raw": è¿”å›åŸå§‹ API å“åº” dict (é»˜è®¤)
        use_provider_api: æ˜¯å¦ä½¿ç”¨å‚å•†åŸå§‹ APIï¼ˆTrue æ—¶ä½¿ç”¨ PROVIDER_API_KEY/BASEï¼‰
        **kwargs: å…¶ä»–å‚æ•°
    
    Returns:
        æ ¹æ® response_type è¿”å›:
        - "content": str (å“åº”å†…å®¹)
        - "raw": Dict (åŸå§‹ JSON æ ¼å¼å“åº”)
    
    Example:
        >>> from message_manager import HumanMessage
        >>> # è·å–åŸå§‹å“åº”
        >>> resp = response(model="gpt-5", messages=[HumanMessage(content="Hello!")])
        >>> print(resp['choices'][0]['message']['content'])
        
        >>> # åªè·å–å†…å®¹
        >>> content = response(model="gpt-5", messages=[HumanMessage(content="Hello!")], response_type="content")
        >>> print(content)  # ç›´æ¥è¾“å‡ºå­—ç¬¦ä¸²
    """
    # è§£ææ¨¡å‹åç§°
    provider = None
    if "/" in model:
        provider, model_name = model.split("/", 1)
        model = model_name
    
    # è‡ªåŠ¨è®¾ç½®ä½¿ç”¨ responses API
    raw_response = model_manager.chat(
        model=model,
        messages=messages,
        tools=tools,
        response_format=response_format,
        stream=stream,
        use_responses_api=True,  # response() å¼ºåˆ¶ä½¿ç”¨ responses API
        use_provider_api=use_provider_api,  # æ˜¯å¦ä½¿ç”¨å‚å•†åŸå§‹ API
        provider=provider,  # ä¼ é€’è§£æå‡ºçš„ provider
        **kwargs
    )
    
    # æµå¼å“åº”ç›´æ¥è¿”å›
    if stream:
        return raw_response
    
    # æ ¹æ® response_type è¿”å›å¯¹åº”æ ¼å¼
    llm_response = LLMResponse(raw_response=raw_response, response_type=response_type)
    return llm_response.get()


if __name__ == "__main__":
    from message_manager import HumanMessage
    import json as json_module
    from datetime import datetime
    
    # ============================================
    # æµ‹è¯•è¾“å‡ºæ—¥å¿—ç±»ï¼ˆåŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶ï¼‰
    # ============================================
    class TestLogger:
        def __init__(self, output_file: str = "test_results.md"):
            self.output_file = Path(__file__).parent / output_file
            self.lines = []
            
        def log(self, message: str = ""):
            """è¾“å‡ºåˆ°æ§åˆ¶å°å¹¶è®°å½•"""
            print(message)
            self.lines.append(message)
        
        def save(self):
            """ä¿å­˜åˆ° md æ–‡ä»¶ï¼ˆè¦†ç›–æ¨¡å¼ï¼‰"""
            with open(self.output_file, 'w', encoding='utf-8') as f:
                # æ·»åŠ æ ‡é¢˜å’Œæ—¶é—´æˆ³
                f.write("# æ¨¡å‹ç®¡ç†å™¨æµ‹è¯•ç»“æœ\n\n")
                f.write(f"**æµ‹è¯•æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                f.write("---\n\n")
                
                # å†™å…¥æ‰€æœ‰æ—¥å¿—
                for line in self.lines:
                    # è½¬æ¢æ ¼å¼ä¸º markdown
                    if line.startswith("="*50):
                        f.write("\n---\n\n")
                    elif "æµ‹è¯•" in line and not line.startswith(" "):
                        f.write(f"## {line}\n\n")
                    elif line.startswith("ğŸ“") or line.startswith("ğŸ“‹") or line.startswith("ğŸ–¼ï¸") or line.startswith("ğŸš€"):
                        f.write(f"### {line}\n\n")
                    elif line.startswith("   "):
                        # ç»“æœè¡Œ
                        f.write(f"```\n{line.strip()}\n```\n\n")
                    elif line.startswith("âš ï¸") or line.startswith("âŒ"):
                        f.write(f"> {line}\n\n")
                    elif line.startswith("ğŸ“"):
                        f.write(f"**{line}**\n\n")
                    else:
                        f.write(f"{line}\n\n")
            
            print(f"\nğŸ“„ æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {self.output_file}")
    
    # åˆå§‹åŒ–æ—¥å¿—
    logger = TestLogger()
    log = logger.log
    
    log("ğŸš€ æ¨¡å‹ç®¡ç†å™¨æµ‹è¯•")
    
    # é€šç”¨æµ‹è¯•æ¶ˆæ¯
    simple_messages = [HumanMessage(content="Say hello in one word")]
    format_messages = [HumanMessage(content="ç”Ÿæˆä¸€ä¸ªè™šæ„äººç‰©çš„ä¿¡æ¯ï¼ŒåŒ…å«å§“åã€å¹´é¾„å’Œçˆ±å¥½ã€‚")]
    
    # é€šç”¨å›¾ç‰‡æ¶ˆæ¯ï¼ˆç½‘ç»œ URLï¼‰
    image_messages = [
        HumanMessage(content=[
            {"type": "text", "text": "è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿè¯·ç”¨ä¸­æ–‡ç®€çŸ­æè¿°ã€‚"},
            {
                "type": "image_url",
                "image_url": {
                    "url": "https://q7.itc.cn/q_70/images03/20250219/6c6b4e75e7e6412999a728d67ba7a8d2.jpeg"
                }
            }
        ])
    ]
    
    # é€šç”¨ç»“æ„åŒ–è¾“å‡ºæ ¼å¼
    structured_format = {
        "type": "json_schema",
        "json_schema": {
            "name": "person_info",
            "strict": True,
            "schema": {
                "type": "object",
                "properties": {
                    "name": {"type": "string", "description": "äººç‰©å§“å"},
                    "age": {"type": "integer", "description": "å¹´é¾„"},
                    "hobbies": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "çˆ±å¥½åˆ—è¡¨"
                    }
                },
                "required": ["name", "age", "hobbies"],
                "additionalProperties": False
            }
        }
    }
    
    # JSON æ ¼å¼ï¼ˆå…¼å®¹ä¸æ”¯æŒ json_schema çš„æ¨¡å‹ï¼‰
    json_format = {"type": "json_object"}
    
    def parse_person(content: str):
        """è§£æäººç‰©ä¿¡æ¯ JSON"""
        try:
            person = json_module.loads(content)
            return f"å§“å={person.get('name')}, å¹´é¾„={person.get('age')}, çˆ±å¥½={person.get('hobbies')}"
        except:
            return content
    
    # ============================================
    # 1ï¸âƒ£ OpenAI æµ‹è¯•
    # ============================================
    log("\n" + "="*50)
    log("1ï¸âƒ£ OpenAI æ¨¡å‹æµ‹è¯•")
    log("="*50)
    
    try:
        log("\nğŸ“ åŸºæœ¬é—®ç­”æµ‹è¯•completions (gpt-4o)...")
        resp = completion(model="openai/gpt-4o", messages=simple_messages, response_type="content")
        log(f"   Response: {resp}")

        log("\nğŸ“ åŸºæœ¬é—®ç­”æµ‹è¯•response (gpt-4o)...")
        resp = response(model="openai/gpt-4o", messages=simple_messages, response_type="content")
        log(f"   Response: {resp}")
        
        log("\nğŸ“‹ ç»“æ„åŒ–è¾“å‡ºæµ‹è¯• (gpt-4o)...")
        resp = completion(
            model="openai/gpt-4o", 
            messages=format_messages, 
            response_format=structured_format,
            response_type="content"
        )
        log(f"   Structured: {parse_person(resp)}")
        
        log("\nğŸ–¼ï¸ å›¾ç‰‡ç†è§£æµ‹è¯•-ç½‘ç»œURL (gpt-5)...")
        resp = response(model="openai/gpt-5", messages=image_messages, response_type="raw")
        log(f"   å›¾ç‰‡æè¿°: {resp}")
        
        log("\nğŸš€ Response API æµ‹è¯• (gpt-5)...")
        resp = response(model="openai/gpt-5", messages=simple_messages, response_type="content")
        log(f"   Response: {resp}")
        
        log("\nğŸ”¬ Deep Research æµå¼æµ‹è¯• (o3-deep-research)...")
        # Deep Research éœ€è¦é…ç½® web_search_preview å·¥å…·
        research_messages = [
            HumanMessage(content="è¯·æ·±å…¥ç ”ç©¶å¹¶æ€»ç»“ï¼šé‡å­è®¡ç®—çš„åŸºæœ¬åŸç†æ˜¯ä»€ä¹ˆï¼Ÿå®ƒä¸ç»å…¸è®¡ç®—æœ‰ä»€ä¹ˆæ ¹æœ¬åŒºåˆ«ï¼Ÿ")
        ]
        # Deep Research å¿…é¡»é…ç½®å·¥å…·ï¼šweb_search_preview, mcp, æˆ– file_search
        deep_research_tools = [
            {"type": "web_search_preview"}  # å¯ç”¨ç½‘é¡µæœç´¢èƒ½åŠ›
        ]
        
        # æµå¼è¾“å‡º
        print("   [æµå¼è¾“å‡ºå¼€å§‹]")
        stream_resp = response(
            model="openai/o3-deep-research",
            messages=research_messages,
            tools=deep_research_tools,
            stream=True,  # å¯ç”¨æµå¼è¾“å‡º
            timeout=600
        )
        
        # å¤„ç†æµå¼å“åº”
        full_content = ""
        for event in stream_resp:
            # æ ¹æ®äº‹ä»¶ç±»å‹å¤„ç†
            if hasattr(event, 'type'):
                if event.type == 'response.output_text.delta':
                    # æ–‡æœ¬å¢é‡
                    delta = event.delta if hasattr(event, 'delta') else ""
                    print(delta, end="", flush=True)
                    full_content += delta
                elif event.type == 'response.completed':
                    # å“åº”å®Œæˆ
                    print("\n   [æµå¼è¾“å‡ºç»“æŸ]")
            elif hasattr(event, 'choices'):
                # chat/completions æµå¼æ ¼å¼
                for choice in event.choices:
                    if hasattr(choice, 'delta') and hasattr(choice.delta, 'content'):
                        delta = choice.delta.content or ""
                        print(delta, end="", flush=True)
                        full_content += delta
        
        log(f"   Research Result (æ€»é•¿åº¦: {len(full_content)} å­—ç¬¦)")
        
        log("\nğŸ–¥ï¸ Computer Use æµ‹è¯• (computer-use-preview)...")
        log("   ğŸ’¡ æç¤º: ä½¿ç”¨ computer_use.py æ¨¡å—è¿›è¡ŒçœŸå®è‡ªåŠ¨åŒ–")
        log("   è¿è¡Œ: python computer_use.py")
        
        # ç®€å•æµ‹è¯•ï¼šåªå‘é€ä¸€æ¬¡è¯·æ±‚çœ‹å“åº”
        computer_tool = {
            "type": "computer_use_preview",
            "display_width": 1920,
            "display_height": 1080,
            "environment": "browser",
        }
        
        resp = response(
            model="openai/computer-use-preview",
            messages=[HumanMessage(content="æˆªå–å½“å‰å±å¹•")],
            tools=[computer_tool],
            response_type="raw",
            timeout=60,
            truncation="auto"
        )
        
        if resp and 'output' in resp:
            for item in resp['output']:
                if item.get('type') == 'computer_call':
                    action = item.get('action', {})
                    log(f"   âœ… Computer Action: {action.get('type', 'unknown')}")
        else:
            log(f"   âš ï¸ å“åº”: {str(resp)[:200]}...")
        
    except Exception as e:
        log(f"   âŒ OpenAI æµ‹è¯•å¤±è´¥: {e}")
    
    # ============================================
    # ğŸ–¼ï¸ æœ¬åœ°å›¾ç‰‡æµ‹è¯•ï¼ˆç‹¬ç«‹æµ‹è¯•å—ï¼‰
    # ============================================
    log("\n" + "="*50)
    log("ğŸ–¼ï¸ æœ¬åœ°å›¾ç‰‡ä¸Šä¼ æµ‹è¯•")
    log("="*50)
    
    try:
        # æµ‹è¯•æœ¬åœ°å›¾ç‰‡è·¯å¾„ï¼ˆè¯·æ›¿æ¢ä¸ºå®é™…å­˜åœ¨çš„å›¾ç‰‡è·¯å¾„ï¼‰
        local_image_path = "./test_image/img1.webp"
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        import os
        if os.path.exists(local_image_path):
            log(f"\nğŸ“ æ‰¾åˆ°æœ¬åœ°å›¾ç‰‡: {local_image_path}")
            
            # ç›´æ¥ä½¿ç”¨ HumanMessage æ„é€ æœ¬åœ°å›¾ç‰‡æ¶ˆæ¯
            local_image_msgs = [
                HumanMessage(content=[
                    {"type": "text", "text": "è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿè¯·ç”¨ä¸­æ–‡ç®€çŸ­æè¿°ã€‚"},
                    {"type": "image_url", "image_url": {"url": local_image_path}}
                ])
            ]
            
            log("\nğŸ–¼ï¸ æœ¬åœ°å›¾ç‰‡æµ‹è¯• (gpt-4o - completion)...")
            resp = completion(model="openai/gpt-4o", messages=local_image_msgs, response_type="content")
            log(f"   å›¾ç‰‡æè¿°: {resp}")
            
            log("\nğŸ–¼ï¸ æœ¬åœ°å›¾ç‰‡æµ‹è¯• (gemini-2.5-pro - completion)...")
            resp = completion(model="gemini-2.5-pro", messages=local_image_msgs, response_type="content")
            log(f"   å›¾ç‰‡æè¿°: {resp}")
        else:
            log(f"\nâš ï¸ æœ¬åœ°å›¾ç‰‡ä¸å­˜åœ¨: {local_image_path}")
            log("   è¯·åˆ›å»ºæµ‹è¯•å›¾ç‰‡æˆ–ä¿®æ”¹ local_image_path å˜é‡")
            
    except Exception as e:
        log(f"   âŒ æœ¬åœ°å›¾ç‰‡æµ‹è¯•å¤±è´¥: {e}")
    
    # ============================================
    # 2ï¸âƒ£ Qwen (é€šä¹‰åƒé—®) æµ‹è¯•
    # ============================================
    log("\n" + "="*50)
    log("2ï¸âƒ£ Qwen (é€šä¹‰åƒé—®) æ¨¡å‹æµ‹è¯•")
    log("="*50)
    
    try:
        log("\nğŸ“ åŸºæœ¬é—®ç­”æµ‹è¯• (qwen-plus)...")
        resp = completion(model="qwen-plus", messages=simple_messages, response_type="content")
        log(f"   Response: {resp}")
        
        log("\nğŸ“‹ ç»“æ„åŒ–è¾“å‡ºæµ‹è¯• (qwen-plus)...")
        # Qwen ä½¿ç”¨ json_object æ ¼å¼
        qwen_format_messages = [HumanMessage(content="ç”Ÿæˆä¸€ä¸ªè™šæ„äººç‰©çš„JSONä¿¡æ¯ï¼ŒåŒ…å«name(å§“å)ã€age(å¹´é¾„)å’Œhobbies(çˆ±å¥½æ•°ç»„)å­—æ®µã€‚åªè¾“å‡ºJSONã€‚")]
        resp = completion(
            model="qwen-plus", 
            messages=qwen_format_messages, 
            response_format=structured_format,
            response_type="content"
        )
        log(f"   Structured: {parse_person(resp)}")
        
        log("\nğŸ–¼ï¸ å›¾ç‰‡ç†è§£æµ‹è¯• (qwen3-vl-plus)...")
        resp = completion(model="qwen3-vl-plus", messages=image_messages, response_type="content")
        log(f"   å›¾ç‰‡æè¿°: {resp}")
        
    except Exception as e:
        log(f"   âŒ Qwen æµ‹è¯•å¤±è´¥: {e}")
    
    # ============================================
    # 3ï¸âƒ£ DeepSeek æµ‹è¯•
    # ============================================
    log("\n" + "="*50)
    log("3ï¸âƒ£ DeepSeek æ¨¡å‹æµ‹è¯•")
    log("="*50)
    
    try:
        log("\nğŸ“ åŸºæœ¬é—®ç­”æµ‹è¯• (deepseek-v3.2-exp)...")
        resp = completion(model="deepseek-v3.2-exp", messages=simple_messages, response_type="content")
        log(f"   Response: {resp}")
        
        log("\nğŸ“‹ ç»“æ„åŒ–è¾“å‡ºæµ‹è¯• (deepseek-v3.2-exp)...")
        deepseek_format_messages = [HumanMessage(content="ç”Ÿæˆä¸€ä¸ªè™šæ„äººç‰©çš„JSONä¿¡æ¯ï¼ŒåŒ…å«name(å§“å)ã€age(å¹´é¾„)å’Œhobbies(çˆ±å¥½æ•°ç»„)å­—æ®µã€‚åªè¾“å‡ºJSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚")]
        resp = completion(
            model="deepseek-v3.2-exp", 
            messages=deepseek_format_messages, 
            response_format=structured_format,
            response_type="content"
        )
        log(f"   Structured: {parse_person(resp)}")
        
    except Exception as e:
        log(f"   âŒ DeepSeek æµ‹è¯•å¤±è´¥: {e}")
    
    # ============================================
    # 4ï¸âƒ£ Claude (Anthropic) æµ‹è¯•
    # ============================================
    log("\n" + "="*50)
    log("4ï¸âƒ£ Claude (Anthropic) æ¨¡å‹æµ‹è¯•")
    log("="*50)
    
    try:
        log("\nğŸ“ åŸºæœ¬é—®ç­”æµ‹è¯• (claude-sonnet-4-5-20250929)...")
        resp = completion(model="claude-sonnet-4-5-20250929", messages=simple_messages, response_type="content")
        log(f"   Response: {resp}")
        
        log("\nğŸ“‹ ç»“æ„åŒ–è¾“å‡ºæµ‹è¯• (claude-sonnet-4-5-20250929)...")
        claude_format_messages = [HumanMessage(content="ç”Ÿæˆä¸€ä¸ªè™šæ„äººç‰©çš„JSONä¿¡æ¯ï¼ŒåŒ…å«name(å§“å)ã€age(å¹´é¾„)å’Œhobbies(çˆ±å¥½æ•°ç»„)å­—æ®µã€‚åªè¾“å‡ºçº¯JSONï¼Œä¸è¦markdownä»£ç å—ã€‚")]
        resp = completion(
            model="claude-sonnet-4-5-20250929", 
            messages=claude_format_messages, 
            response_type="content",
            response_format=structured_format
        )
        log(f"   Structured: {parse_person(resp)}")
        
    except Exception as e:
        log(f"   âŒ Claude æµ‹è¯•å¤±è´¥: {e}")
    
    # ============================================
    # 5ï¸âƒ£ Gemini (Google) æµ‹è¯•
    # ============================================
    log("\n" + "="*50)
    log("5ï¸âƒ£ Gemini (Google) æ¨¡å‹æµ‹è¯•")
    log("="*50)
    
    try:
        log("\nğŸ“ åŸºæœ¬é—®ç­”æµ‹è¯• (gemini-2.5-pro)...")
        resp = completion(model="gemini-2.5-pro", messages=simple_messages, response_type="content")
        log(f"   Response: {resp}")
        
        log("\nğŸ“‹ ç»“æ„åŒ–è¾“å‡ºæµ‹è¯• (gemini-2.5-pro)...")
        gemini_format_messages = [HumanMessage(content="ç”Ÿæˆä¸€ä¸ªè™šæ„äººç‰©çš„JSONä¿¡æ¯ï¼ŒåŒ…å«name(å§“å)ã€age(å¹´é¾„)å’Œhobbies(çˆ±å¥½æ•°ç»„)å­—æ®µã€‚åªè¾“å‡ºçº¯JSONã€‚")]
        resp = completion(
            model="gemini-2.5-pro", 
            messages=gemini_format_messages, 
            response_format=structured_format,
            response_type="content"
        )
        log(f"   Structured: {parse_person(resp)}")
        
        log("\nğŸ–¼ï¸ å›¾ç‰‡ç†è§£æµ‹è¯• (gemini-2.5-pro)...")
        resp = completion(model="gemini-2.5-pro", messages=image_messages, response_type="content")
        log(f"   å›¾ç‰‡æè¿°: {resp}")
        
    except Exception as e:
        log(f"   âŒ Gemini æµ‹è¯•å¤±è´¥: {e}")
    
    # ============================================
    # 6. Computer Use æµ‹è¯•ï¼ˆç®€åŒ–ç‰ˆï¼šåªæµ‹è¯•ç‚¹å‡»ä½ç½®ï¼‰
    # ============================================
    log("\n" + "="*50)
    log("6. Computer Use æµ‹è¯• (ç‚¹å‡»ä½ç½®æµ‹è¯•)")
    log("="*50)
    
    try:
        from PIL import Image
        
        # æµ‹è¯•å›¾ç‰‡è·¯å¾„ï¼ˆ1024x768 çš„æˆªå›¾ï¼‰
        test_screenshot = "./test_image/screenshot.png"
        
        if os.path.exists(test_screenshot):
            log(f"\næ‰¾åˆ°æµ‹è¯•æˆªå›¾: {test_screenshot}")
            
            # éªŒè¯å›¾ç‰‡å°ºå¯¸
            with Image.open(test_screenshot) as img:
                width, height = img.size
                log(f"   å›¾ç‰‡å°ºå¯¸: {width}x{height}")
            
            # ä½¿ç”¨ HumanMessage æ„é€ æ¶ˆæ¯ï¼ˆæœ¬åœ°å›¾ç‰‡ä¼šè‡ªåŠ¨è½¬æ¢ä¸º base64ï¼‰
            computer_messages = [
                HumanMessage(content=[
                    {"type": "text", "text": "è¯·åœ¨å±å¹•ä¸Šæ‰¾åˆ°å¹¶ç‚¹å‡» model.json æ–‡ä»¶"},
                    {"type": "image_url", "image_url": {"url": test_screenshot}}
                ])
            ]
            
            # Computer Use å·¥å…·é…ç½®
            computer_tool = {
                "type": "computer_use_preview",
                "display_width": 1024,
                "display_height": 768,
                "environment": "mac"
            }
            
            log("\nComputer Use ç‚¹å‡»æµ‹è¯• (computer-use-preview)...")
            log("   ä»»åŠ¡: ç‚¹å‡» model.json æ–‡ä»¶")
            
            # ä½¿ç”¨ response() å‡½æ•°è°ƒç”¨ APIï¼ˆä½¿ç”¨å‚å•†åŸå§‹ APIï¼‰
            resp = response(
                model="openai/computer-use-preview",
                messages=computer_messages,
                tools=[computer_tool],
                response_type="raw",
                use_provider_api=True,  # ä½¿ç”¨ OpenAI åŸå§‹ API
                truncation="auto",
                timeout=60
            )
            
            # è§£æå“åº”ï¼Œè·å–ç‚¹å‡»ä½ç½®
            if resp and 'output' in resp:
                for item in resp['output']:
                    if item.get('type') == 'computer_call':
                        action = item.get('action', {})
                        action_type = action.get('type', 'unknown')
                        
                        if action_type == 'click':
                            x = action.get('x', 0)
                            y = action.get('y', 0)
                            button = action.get('button', 'left')
                            log(f"   æ¨¡å‹è¿”å›ç‚¹å‡»åŠ¨ä½œ:")
                            log(f"      ä½ç½®: ({x}, {y})")
                            log(f"      æŒ‰é’®: {button}")
                        elif action_type == 'screenshot':
                            log(f"   æ¨¡å‹è¯·æ±‚æˆªå›¾")
                        else:
                            log(f"   åŠ¨ä½œç±»å‹: {action_type}, è¯¦æƒ…: {action}")
                    elif item.get('type') == 'message':
                        for content in item.get('content', []):
                            if content.get('type') == 'output_text':
                                log(f"   æ¨¡å‹æ¶ˆæ¯: {content.get('text', '')[:100]}")
            else:
                log(f"   å“åº”æ ¼å¼å¼‚å¸¸: {str(resp)[:200]}...")
                
        else:
            log(f"\næµ‹è¯•æˆªå›¾ä¸å­˜åœ¨: {test_screenshot}")
            log("   è¯·å…ˆæˆªå–ä¸€å¼  1024x768 çš„å±å¹•æˆªå›¾å¹¶ä¿å­˜åˆ° test_image/screenshot.png")
            
    except Exception as e:
        log(f"   Computer Use æµ‹è¯•å¤±è´¥: {e}")
    
    # ============================================
    # 7. Anthropic Computer Use æµ‹è¯•
    # ============================================
    # log("\n" + "="*50)
    # log("7. Anthropic Computer Use æµ‹è¯• (ç‚¹å‡»ä½ç½®æµ‹è¯•)")
    # log("="*50)
    
    # try:
    #     from PIL import Image
        
    #     # æµ‹è¯•å›¾ç‰‡è·¯å¾„ï¼ˆ1024x768 çš„æˆªå›¾ï¼‰
    #     test_screenshot = "./test_image/screenshot.png"
        
    #     if os.path.exists(test_screenshot):
    #         log(f"\næ‰¾åˆ°æµ‹è¯•æˆªå›¾: {test_screenshot}")
            
    #         # éªŒè¯å›¾ç‰‡å°ºå¯¸
    #         with Image.open(test_screenshot) as img:
    #             width, height = img.size
    #             log(f"   å›¾ç‰‡å°ºå¯¸: {width}x{height}")
            
    #         # ä½¿ç”¨ HumanMessage æ„é€ æ¶ˆæ¯ï¼ˆæœ¬åœ°å›¾ç‰‡è‡ªåŠ¨è½¬æ¢ä¸º base64ï¼‰
    #         computer_messages = [
    #             HumanMessage(content=[
    #                 {"type": "text", "text": "è¯·åœ¨å±å¹•ä¸Šæ‰¾åˆ°å¹¶ç‚¹å‡» model.json æ–‡ä»¶"},
    #                 {"type": "image_url", "image_url": {"url": test_screenshot}}
    #             ])
    #         ]
            
    #         # Anthropic Computer Use å·¥å…·é…ç½®
    #         computer_tool = {
    #             "type": "computer_20250124",
    #             "name": "computer",
    #             "display_width_px": 1024,
    #             "display_height_px": 768,
    #             "display_number": 1,
    #         }
            
    #         log("\nAnthropic Computer Use ç‚¹å‡»æµ‹è¯•...")
    #         log("   ä»»åŠ¡: ç‚¹å‡» model.json æ–‡ä»¶")
            
    #         # ä½¿ç”¨ completion() å‡½æ•°è°ƒç”¨ APIï¼ˆä½¿ç”¨å‚å•†åŸå§‹ APIï¼‰
    #         resp = completion(
    #             model="anthropic/computer-use-2025-11-24",
    #             messages=computer_messages,
    #             tools=[computer_tool],
    #             response_type="raw",
    #             use_provider_api=True,  # ä½¿ç”¨ Anthropic åŸå§‹ API
    #             timeout=60
    #         )
            
    #         # è°ƒè¯•ï¼šæ‰“å°åŸå§‹å“åº”
    #         log(f"   åŸå§‹å“åº”: {json_module.dumps(resp, ensure_ascii=False, indent=2)[:500] if resp else 'None'}...")
            
    #         # è§£æå“åº”ï¼Œè·å–ç‚¹å‡»ä½ç½®
    #         if resp and 'choices' in resp:
    #             message = resp['choices'][0].get('message', {})
    #             tool_calls = message.get('tool_calls', [])
                
    #             if tool_calls:
    #                 for tool_call in tool_calls:
    #                     func_name = tool_call.get('function', {}).get('name', '')
    #                     if func_name == 'computer':
    #                         args = json_module.loads(tool_call['function'].get('arguments', '{}'))
    #                         action = args.get('action', '')
                            
    #                         if action == 'left_click':
    #                             coords = args.get('coordinate', [0, 0])
    #                             log(f"   æ¨¡å‹è¿”å›ç‚¹å‡»åŠ¨ä½œ:")
    #                             log(f"      ä½ç½®: ({coords[0]}, {coords[1]})")
    #                             log(f"      åŠ¨ä½œ: left_click")
    #                         elif action == 'screenshot':
    #                             log(f"   æ¨¡å‹è¯·æ±‚æˆªå›¾")
    #                         else:
    #                             log(f"   åŠ¨ä½œç±»å‹: {action}, è¯¦æƒ…: {args}")
    #             else:
    #                 # æ²¡æœ‰ tool_callsï¼ŒæŸ¥çœ‹æ–‡æœ¬å†…å®¹
    #                 content = message.get('content', '')
    #                 if content:
    #                     log(f"   æ¨¡å‹æ¶ˆæ¯: {content[:200]}")
    #         elif resp:
    #             log(f"   å“åº”æ ¼å¼: {list(resp.keys()) if isinstance(resp, dict) else type(resp)}")
    #         else:
    #             log(f"   å“åº”ä¸ºç©º")
                
    #     else:
    #         log(f"\næµ‹è¯•æˆªå›¾ä¸å­˜åœ¨: {test_screenshot}")
    #         log("   è¯·å…ˆæˆªå–ä¸€å¼  1024x768 çš„å±å¹•æˆªå›¾å¹¶ä¿å­˜åˆ° test_image/screenshot.png")
            
    # except Exception as e:
    #     log(f"   Anthropic Computer Use æµ‹è¯•å¤±è´¥: {e}")
    
    log("\n" + "="*50)
    log("æµ‹è¯•å®Œæˆ")
    log("="*50)
    
    # ä¿å­˜æµ‹è¯•ç»“æœåˆ° md æ–‡ä»¶
    logger.save()
