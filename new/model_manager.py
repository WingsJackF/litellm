"""
ç®€åŒ–ç‰ˆæ¨¡å‹ç®¡ç†å™¨
åŸºäº LiteLLM çš„è®¾è®¡æ€è·¯ï¼Œå®ç°æ¨¡å‹çš„æ³¨å†Œã€è¯†åˆ«å’Œç®¡ç†åŠŸèƒ½
æ”¯æŒç»Ÿä¸€çš„ API è°ƒç”¨æ¥å£ (model, messages, tools, response_format)
"""

import os
import json
from pathlib import Path
from typing import Optional, Dict, List, Any, Union
from dataclasses import dataclass, field, asdict
from dotenv import load_dotenv
from openai import OpenAI

# åŠ è½½ .env æ–‡ä»¶ï¼ˆå§‹ç»ˆä»å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•åŠ è½½ï¼‰
_env_path = Path(__file__).parent / ".env"
load_dotenv(_env_path, override=True)  # override=True ç¡®ä¿ .env ä¼˜å…ˆäºç³»ç»Ÿç¯å¢ƒå˜é‡


@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®ç±»"""
    model_name: str
    provider: str
    api_base: Optional[str] = None
    api_key: Optional[str] = None
    default_params: Dict = field(default_factory=dict)
    supports_streaming: bool = True
    supports_functions: bool = False
    supports_vision: bool = False
    max_tokens: Optional[int] = None
    
    # æ–°å¢å­—æ®µï¼Œç”¨äºæŒ‡ç¤ºæ˜¯å¦ä½¿ç”¨ç‰¹æ®Šçš„ responses API (å¦‚ gpt-5)
    use_responses_api: bool = False
    output_version: Optional[str] = None


@dataclass
class LLMResponse:
    """
    LLM å“åº”åŒ…è£…ç±»
    
    æ ¹æ® response_type è¿”å›ä¸åŒæ ¼å¼çš„æ•°æ®ï¼š
    - "content": åªè¿”å›å“åº”å†…å®¹ (å­—ç¬¦ä¸²)
    - "raw": è¿”å›åŸå§‹ API å“åº” (å®Œæ•´ dict)
    
    æ”¯æŒä¸¤ç§ API å“åº”æ ¼å¼ï¼š
    - chat/completions: choices[0].message.content
    - responses API: output[0].content[0].text
    
    Example:
        >>> resp = LLMResponse(raw_response=api_result, response_type="content")
        >>> print(resp.get())  # åªè¿”å›å†…å®¹å­—ç¬¦ä¸²
        
        >>> resp = LLMResponse(raw_response=api_result, response_type="raw")
        >>> print(resp.get())  # è¿”å›å®Œæ•´çš„åŸå§‹å“åº”
    """
    raw_response: Dict[str, Any]
    response_type: str = "content"  # "content" æˆ– "raw"
    
    @property
    def content(self) -> str:
        """è·å–å“åº”å†…å®¹ï¼ˆè‡ªåŠ¨é€‚é…ä¸åŒ API æ ¼å¼ï¼‰"""
        if self.raw_response is None:
            return ""
        
        try:
            # æ ¼å¼1: æ ‡å‡† chat/completions API
            # {"choices": [{"message": {"content": "..."}}]}
            if 'choices' in self.raw_response:
                return self.raw_response['choices'][0]['message']['content'] or ""
            
            # æ ¼å¼2: OpenAI responses API (å¦‚ o1, o3, o4-mini ç­‰)
            # {"output": [{"type": "message", "content": [{"type": "output_text", "text": "..."}]}]}
            if 'output' in self.raw_response:
                for item in self.raw_response['output']:
                    if item.get('type') == 'message':
                        for content_item in item.get('content', []):
                            if content_item.get('type') == 'output_text':
                                return content_item.get('text', '')
            
            return ""
        except (KeyError, IndexError, TypeError):
            return ""
    
    @property
    def raw(self) -> Dict[str, Any]:
        """è·å–åŸå§‹å“åº”"""
        return self.raw_response
    
    @property
    def usage(self) -> Optional[Dict[str, int]]:
        """è·å– token ä½¿ç”¨æƒ…å†µ"""
        if self.raw_response is None:
            return None
        return self.raw_response.get('usage')
    
    @property
    def model(self) -> Optional[str]:
        """è·å–å®é™…ä½¿ç”¨çš„æ¨¡å‹åç§°"""
        if self.raw_response is None:
            return None
        return self.raw_response.get('model')
    
    def get(self) -> Union[str, Dict[str, Any]]:
        """æ ¹æ® response_type è¿”å›å¯¹åº”æ•°æ®"""
        if self.response_type == "raw":
            return self.raw_response
        return self.content
    
    def __str__(self) -> str:
        """å­—ç¬¦ä¸²è¡¨ç¤ºï¼Œè¿”å›å†…å®¹"""
        return self.content
    
    def __repr__(self) -> str:
        content_preview = self.content[:50] if self.content else ""
        return f"LLMResponse(response_type='{self.response_type}', content='{content_preview}...')"


class ModelManager:
    """
    æ¨¡å‹ç®¡ç†å™¨ - ç®¡ç†å’Œè¯†åˆ«ä¸åŒçš„ LLM æ¨¡å‹
    
    åŠŸèƒ½ï¼š
    1. æ³¨å†Œå’Œç®¡ç†æ¨¡å‹é…ç½®
    2. ç»Ÿä¸€çš„ API è°ƒç”¨æ¥å£ (chat)
    """
    
    def __init__(self, model_file: str = "model.json"):
        """
        åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨
        
        Args:
            model_file: æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸º model.json
        """
        self.model_file = Path(__file__).parent / model_file
        self.models: Dict[str, ModelConfig] = {}
        self.model_aliases: Dict[str, str] = {}
        
        # æä¾›å•†åˆ—è¡¨
        self.providers: List[str] = [
            "openai", "anthropic", "google", "deepseek", "ollama"
        ]
        
        # æä¾›å•†é»˜è®¤ API Base URL æ˜ å°„
        self.provider_api_bases: Dict[str, str] = {
            "openai": "https://api.openai.com/v1",
            "anthropic": "https://api.anthropic.com",
            "google": "https://generativelanguage.googleapis.com/v1",
            "deepseek": "https://api.deepseek.com/v1",
            "ollama": "http://localhost:11434/v1",
        }
        
        # å·²çŸ¥æ¨¡å‹é…ç½®
        self.known_models: Dict[str, str] = {
            "gpt-4o": "openai",
            "gpt-4-turbo": "openai",
            "claude-3-5-sonnet-20241022": "anthropic",
            "gemini-1.5-pro": "google",
            "deepseek-chat": "deepseek"
        }
        
        self._initialize_default_models()
        self._load_from_json()
    
    def _initialize_default_models(self):
        """åˆå§‹åŒ–é»˜è®¤æ”¯æŒçš„æ¨¡å‹é…ç½®"""
        for model_name, provider in self.known_models.items():
            config = ModelConfig(
                model_name=model_name,
                provider=provider,
                api_base=self.provider_api_bases.get(provider),
                supports_vision="gpt-4" in model_name or "claude" in model_name or "gemini" in model_name
            )
            self.models[model_name] = config
    
    def _load_from_json(self):
        """ä» JSON æ–‡ä»¶åŠ è½½è‡ªå®šä¹‰æ¨¡å‹é…ç½®"""
        try:
            if self.model_file.exists():
                with open(self.model_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                if 'custom_models' in data:
                    for model_data in data['custom_models']:
                        config = ModelConfig(
                            model_name=model_data['model_name'],
                            provider=model_data['provider'],
                            api_base=model_data.get('api_base'),
                            api_key=model_data.get('api_key'),
                            default_params=model_data.get('default_params', {}),
                            supports_streaming=model_data.get('supports_streaming', True),
                            supports_functions=model_data.get('supports_functions', False),
                            supports_vision=model_data.get('supports_vision', False),
                            max_tokens=model_data.get('max_tokens'),
                            use_responses_api=model_data.get('use_responses_api', False),
                            output_version=model_data.get('output_version')
                        )
                        self.models[model_data['model_name']] = config
                
                if 'aliases' in data:
                    self.model_aliases = data['aliases']
                    
        except Exception as e:
            print(f"âš ï¸  åŠ è½½æ¨¡å‹é…ç½®å¤±è´¥: {e}")
    
    def _save_to_json(self):
        """ä¿å­˜è‡ªå®šä¹‰æ¨¡å‹é…ç½®åˆ° JSON æ–‡ä»¶"""
        try:
            custom_models = []
            for model_name, config in self.models.items():
                if model_name not in self.known_models:
                    model_data = asdict(config)
                    custom_models.append(model_data)
            
            data = {
                'custom_models': custom_models,
                'aliases': self.model_aliases
            }
            
            with open(self.model_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âŒ ä¿å­˜æ¨¡å‹é…ç½®å¤±è´¥: {e}")

    def register_model(self, model_name: str, provider: str, **kwargs):
        """æ³¨å†Œæ–°æ¨¡å‹"""
        # æå– api_baseï¼Œé¿å…é‡å¤ä¼ é€’
        api_base = kwargs.pop('api_base', None) or self.provider_api_bases.get(provider)
        
        config = ModelConfig(
            model_name=model_name,
            provider=provider,
            api_base=api_base,
            **kwargs
        )
        self.models[model_name] = config
        self._save_to_json()
        return config

    def get_model_config(self, model: str) -> Optional[ModelConfig]:
        """è·å–æ¨¡å‹é…ç½®"""
        if model in self.model_aliases:
            model = self.model_aliases[model]
        return self.models.get(model)

    def _get_api_key(self, provider: str, config_key: Optional[str] = None, use_provider_specific: bool = False) -> Optional[str]:
        """
        è·å– API Key
        
        Args:
            provider: æä¾›å•†åç§°
            config_key: é…ç½®ä¸­çš„ key
            use_provider_specific: æ˜¯å¦ä¼˜å…ˆä½¿ç”¨æä¾›å•†ç‰¹å®šçš„ç¯å¢ƒå˜é‡ï¼ˆç”¨äº responses APIï¼‰
        """
        # æä¾›å•†ç‰¹å®šçš„ç¯å¢ƒå˜é‡æ˜ å°„
        env_keys = {
            "openai": "OPENAI_API_KEY",
            "anthropic": "ANTHROPIC_API_KEY",
            "google": "GOOGLE_API_KEY",
            "deepseek": "DEEPSEEK_API_KEY",
        }
        
        if use_provider_specific:
            # responses API: ä¼˜å…ˆä½¿ç”¨æä¾›å•†ç‰¹å®šçš„ç¯å¢ƒå˜é‡
            # 1. æä¾›å•†ç‰¹å®šçš„ç¯å¢ƒå˜é‡
            provider_key = os.getenv(env_keys.get(provider, ""))
            if provider_key:
                return provider_key
            # 2. é…ç½®ä¸­çš„ key
            if config_key:
                return config_key
            # 3. ç»Ÿä¸€çš„ API_KEY ä½œä¸ºåå¤‡
            return os.getenv("API_KEY")
        else:
            # completion API: ä¼˜å…ˆä½¿ç”¨ç»Ÿä¸€çš„ API_KEYï¼ˆä»£ç†ï¼‰
            # 1. ç»Ÿä¸€çš„ API_KEY
            unified_key = os.getenv("API_KEY")
            if unified_key:
                return unified_key
            # 2. é…ç½®ä¸­çš„ key
            if config_key:
                return config_key
            # 3. æä¾›å•†ç‰¹å®šçš„ç¯å¢ƒå˜é‡
            return os.getenv(env_keys.get(provider, ""))

    def _get_api_base(self, provider: str, config_base: Optional[str] = None, use_provider_specific: bool = False) -> Optional[str]:
        """
        è·å– API Base
        
        Args:
            provider: æä¾›å•†åç§°
            config_base: é…ç½®ä¸­çš„ base
            use_provider_specific: æ˜¯å¦ä¼˜å…ˆä½¿ç”¨æä¾›å•†ç‰¹å®šçš„ç¯å¢ƒå˜é‡ï¼ˆç”¨äº responses APIï¼‰
        """
        # æä¾›å•†ç‰¹å®šçš„ç¯å¢ƒå˜é‡
        env_var = f"{provider.upper()}_API_BASE"
        
        if use_provider_specific:
            # responses API: ä¼˜å…ˆä½¿ç”¨æä¾›å•†ç‰¹å®šçš„ç¯å¢ƒå˜é‡
            # 1. æä¾›å•†ç‰¹å®šçš„ç¯å¢ƒå˜é‡
            provider_base = os.getenv(env_var)
            if provider_base:
                return provider_base
            # 2. é…ç½®ä¸­çš„ base
            if config_base:
                return config_base
            # 3. é»˜è®¤çš„æä¾›å•† API base
            return self.provider_api_bases.get(provider)
        else:
            # completion API: ä¼˜å…ˆä½¿ç”¨ç»Ÿä¸€çš„ BASE_URLï¼ˆä»£ç†ï¼‰
            # 1. ç»Ÿä¸€çš„ BASE_URL
            unified_base = os.getenv("BASE_URL")
            if unified_base:
                return unified_base
            # 2. æä¾›å•†ç‰¹å®šçš„ç¯å¢ƒå˜é‡
            env_base = os.getenv(env_var)
            if env_base:
                return env_base
            # 3. é…ç½®ä¸­çš„ base
            return config_base

    def chat(
        self, 
        model: str, 
        messages: List[Any], 
        tools: Optional[List[Dict]] = None,
        response_format: Optional[Dict] = None,
        stream: bool = False,
        use_responses_api: Optional[bool] = None,
        **kwargs
    ) -> Union[Dict, Any]:
        """
        ç»Ÿä¸€ API è°ƒç”¨æ¥å£ - ä½¿ç”¨ OpenAI SDK
        
        Args:
            model: æ¨¡å‹åç§°
            messages: æ¶ˆæ¯åˆ—è¡¨ (HumanMessage, AIMessage, SystemMessage å¯¹è±¡)
            tools: å·¥å…·å®šä¹‰åˆ—è¡¨
            response_format: å“åº”æ ¼å¼å®šä¹‰
            stream: æ˜¯å¦æµå¼è¾“å‡º
            use_responses_api: æ˜¯å¦ä½¿ç”¨ responses APIï¼ˆNone æ—¶ä½¿ç”¨é…ç½®æ–‡ä»¶è®¾ç½®ï¼‰
            **kwargs: å…¶ä»–å‚æ•°
        
        Returns:
            Dict: åŸå§‹ OpenAI API æ ¼å¼çš„ JSON å“åº”ï¼ˆéæµå¼ï¼‰
            Stream: æµå¼å“åº”å¯¹è±¡ï¼ˆæµå¼ï¼‰
            æ ¼å¼: {
                "id": "chatcmpl-xxx",
                "model": "gpt-4o",
                "choices": [{"message": {"content": "...", "role": "assistant"}}],
                "usage": {"prompt_tokens": 10, "completion_tokens": 20, "total_tokens": 30}
            }
        """
        # è·å–æ¨¡å‹é…ç½®
        config = self.get_model_config(model) or ModelConfig(model, "openai")
        
        # ç¡®å®šä½¿ç”¨å“ªç§ APIï¼šä¼˜å…ˆä½¿ç”¨å‚æ•°ï¼Œå…¶æ¬¡ä½¿ç”¨é…ç½®
        if use_responses_api is None:
            use_responses_api = config.use_responses_api
        
        # æ ¹æ® API ç±»å‹å†³å®šè·å– key å’Œ base çš„ä¼˜å…ˆçº§
        # responses API: ä¼˜å…ˆä½¿ç”¨æä¾›å•†ç‰¹å®šçš„ç¯å¢ƒå˜é‡ (OPENAI_API_KEY, OPENAI_API_BASE)
        # completion API: ä¼˜å…ˆä½¿ç”¨ç»Ÿä¸€çš„ä»£ç† (API_KEY, BASE_URL)
        api_key = self._get_api_key(config.provider, config.api_key, use_provider_specific=use_responses_api)
        api_base = self._get_api_base(config.provider, config.api_base, use_provider_specific=use_responses_api)
        
        # è½¬æ¢ Messages ä¸º API æ ¼å¼
        from message_manager import MessageManager
        msg_manager = MessageManager(
            api_type="responses" if use_responses_api else "chat/completions",
            model=model
        )
        api_messages = msg_manager(messages)
        
        # åˆ›å»º OpenAI å®¢æˆ·ç«¯ï¼ˆæ·»åŠ è¶…æ—¶è®¾ç½®ï¼‰
        timeout = kwargs.pop('timeout', 120)  # é»˜è®¤ 120 ç§’è¶…æ—¶
        client = OpenAI(
            api_key=api_key,
            base_url=api_base,
            timeout=timeout
        )
        
        # æ ¹æ® API ç±»å‹é€‰æ‹©ä¸åŒçš„è°ƒç”¨æ–¹å¼
        if use_responses_api:
            # ä½¿ç”¨ responses APIï¼ˆå¦‚ GPT-5, o1, o3 ç­‰ï¼‰
            # ç«¯ç‚¹: /responses
            params = {
                "model": model,
                "input": api_messages,  # responses API ä½¿ç”¨ input è€Œä¸æ˜¯ messages
                "stream": stream,
                **kwargs
            }
            
            # æ·»åŠ å¯é€‰å‚æ•°
            if tools:
                params["tools"] = tools
            if config.max_tokens:
                params["max_tokens"] = config.max_tokens
            
            # è°ƒè¯•è¾“å‡º
            print(f"ğŸ”„ Calling API: {api_base}/responses")
            print(f"   Model: {model}, Timeout: {timeout}s")
            
            # è°ƒç”¨ responses API
            response = client.responses.create(**params)
        else:
            # ä½¿ç”¨æ ‡å‡† chat/completions API
            params = {
                "model": model,
                "messages": api_messages,
                "stream": stream,
                **kwargs
            }
            
            # æ·»åŠ å¯é€‰å‚æ•°
            if tools:
                params["tools"] = tools
            if response_format:
                params["response_format"] = response_format
            if config.max_tokens:
                params["max_tokens"] = config.max_tokens
            
            # è°ƒè¯•è¾“å‡º
            print(f"ğŸ”„ Calling API: {api_base}/chat/completions")
            print(f"   Model: {model}, Timeout: {timeout}s")
            
            # è°ƒç”¨ chat/completions API
            response = client.chat.completions.create(**params)
        
        # æµå¼å“åº”ç›´æ¥è¿”å›
        if stream:
            return response
        
        # éæµå¼å“åº”è½¬æ¢ä¸º dict
        return response.model_dump()

# å…¨å±€å®ä¾‹
model_manager = ModelManager()


def completion(
    model: str,
    messages: List[Any],
    tools: Optional[List[Dict]] = None,
    response_format: Optional[Dict] = None,
    stream: bool = False,
    response_type: str = "raw",
    **kwargs
) -> Union[str, Dict, LLMResponse]:
    """
    Completion API è°ƒç”¨ (æ ‡å‡† chat/completions æ¥å£)
    
    é€‚ç”¨äºå¤§å¤šæ•°æ¨¡å‹ï¼šGPT-4, Claude, Gemini, DeepSeek ç­‰
    è‡ªåŠ¨ä½¿ç”¨ chat/completions API ç«¯ç‚¹
    
    Args:
        model: æ¨¡å‹åç§°ï¼Œæ ¼å¼ä¸º "provider/model" æˆ– "model"
               ä¾‹å¦‚: "openai/gpt-4o", "gpt-4o", "anthropic/claude-3-5-sonnet-20241022"
        messages: æ¶ˆæ¯åˆ—è¡¨ (HumanMessage, AIMessage, SystemMessage å¯¹è±¡)
        tools: å·¥å…·å®šä¹‰åˆ—è¡¨
        response_format: å“åº”æ ¼å¼å®šä¹‰
        stream: æ˜¯å¦æµå¼è¾“å‡º
        response_type: å“åº”ç±»å‹
            - "content": åªè¿”å›å†…å®¹å­—ç¬¦ä¸²
            - "raw": è¿”å›åŸå§‹ API å“åº” dict (é»˜è®¤)
        **kwargs: å…¶ä»–å‚æ•°
    
    Returns:
        æ ¹æ® response_type è¿”å›:
        - "content": str (å“åº”å†…å®¹)
        - "raw": Dict (åŸå§‹ JSON æ ¼å¼å“åº”)
    
    Example:
        >>> from message_manager import HumanMessage
        >>> # è·å–åŸå§‹å“åº”
        >>> resp = completion(model="gpt-4o", messages=[HumanMessage(content="Hello!")])
        >>> print(resp['choices'][0]['message']['content'])
        
        >>> # åªè·å–å†…å®¹
        >>> content = completion(model="gpt-4o", messages=[HumanMessage(content="Hello!")], response_type="content")
        >>> print(content)  # ç›´æ¥è¾“å‡ºå­—ç¬¦ä¸²
    """
    # è§£ææ¨¡å‹åç§° (æ”¯æŒ provider/model æ ¼å¼)
    if "/" in model:
        provider, model_name = model.split("/", 1)
        model = model_name
    
    # è‡ªåŠ¨è®¾ç½®ä½¿ç”¨ chat/completions API
    raw_response = model_manager.chat(
        model=model,
        messages=messages,
        tools=tools,
        response_format=response_format,
        stream=stream,
        use_responses_api=False,  # completion() å¼ºåˆ¶ä½¿ç”¨ chat/completions
        **kwargs
    )
    
    # æµå¼å“åº”ç›´æ¥è¿”å›
    if stream:
        return raw_response
    
    # æ ¹æ® response_type è¿”å›å¯¹åº”æ ¼å¼
    llm_response = LLMResponse(raw_response=raw_response, response_type=response_type)
    return llm_response.get()


def response(
    model: str,
    messages: List[Any],
    tools: Optional[List[Dict]] = None,
    response_format: Optional[Dict] = None,
    stream: bool = False,
    response_type: str = "raw",
    **kwargs
) -> Union[str, Dict, LLMResponse]:
    """
    Response API è°ƒç”¨ (æ–°ç‰ˆ responses æ¥å£ï¼Œå¦‚ GPT-5)
    
    é€‚ç”¨äºä½¿ç”¨ responses API çš„æ¨¡å‹ï¼ˆå¦‚ gpt-5ï¼‰
    è‡ªåŠ¨ä½¿ç”¨ responses API ç«¯ç‚¹
    
    Args:
        model: æ¨¡å‹åç§°ï¼Œæ ¼å¼ä¸º "provider/model" æˆ– "model"
               ä¾‹å¦‚: "openai/gpt-5", "gpt-5"
        messages: æ¶ˆæ¯åˆ—è¡¨ (HumanMessage, AIMessage, SystemMessage å¯¹è±¡)
        tools: å·¥å…·å®šä¹‰åˆ—è¡¨
        response_format: å“åº”æ ¼å¼å®šä¹‰
        stream: æ˜¯å¦æµå¼è¾“å‡º
        response_type: å“åº”ç±»å‹
            - "content": åªè¿”å›å†…å®¹å­—ç¬¦ä¸²
            - "raw": è¿”å›åŸå§‹ API å“åº” dict (é»˜è®¤)
        **kwargs: å…¶ä»–å‚æ•°
    
    Returns:
        æ ¹æ® response_type è¿”å›:
        - "content": str (å“åº”å†…å®¹)
        - "raw": Dict (åŸå§‹ JSON æ ¼å¼å“åº”)
    
    Example:
        >>> from message_manager import HumanMessage
        >>> # è·å–åŸå§‹å“åº”
        >>> resp = response(model="gpt-5", messages=[HumanMessage(content="Hello!")])
        >>> print(resp['choices'][0]['message']['content'])
        
        >>> # åªè·å–å†…å®¹
        >>> content = response(model="gpt-5", messages=[HumanMessage(content="Hello!")], response_type="content")
        >>> print(content)  # ç›´æ¥è¾“å‡ºå­—ç¬¦ä¸²
    """
    # è§£ææ¨¡å‹åç§°
    if "/" in model:
        provider, model_name = model.split("/", 1)
        model = model_name
    
    # è‡ªåŠ¨è®¾ç½®ä½¿ç”¨ responses API
    raw_response = model_manager.chat(
        model=model,
        messages=messages,
        tools=tools,
        response_format=response_format,
        stream=stream,
        use_responses_api=True,  # response() å¼ºåˆ¶ä½¿ç”¨ responses API
        **kwargs
    )
    
    # æµå¼å“åº”ç›´æ¥è¿”å›
    if stream:
        return raw_response
    
    # æ ¹æ® response_type è¿”å›å¯¹åº”æ ¼å¼
    llm_response = LLMResponse(raw_response=raw_response, response_type=response_type)
    return llm_response.get()


if __name__ == "__main__":
    from message_manager import HumanMessage
    import json as json_module
    from datetime import datetime
    
    # ============================================
    # æµ‹è¯•è¾“å‡ºæ—¥å¿—ç±»ï¼ˆåŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶ï¼‰
    # ============================================
    class TestLogger:
        def __init__(self, output_file: str = "test_results.md"):
            self.output_file = Path(__file__).parent / output_file
            self.lines = []
            
        def log(self, message: str = ""):
            """è¾“å‡ºåˆ°æ§åˆ¶å°å¹¶è®°å½•"""
            print(message)
            self.lines.append(message)
        
        def save(self):
            """ä¿å­˜åˆ° md æ–‡ä»¶ï¼ˆè¦†ç›–æ¨¡å¼ï¼‰"""
            with open(self.output_file, 'w', encoding='utf-8') as f:
                # æ·»åŠ æ ‡é¢˜å’Œæ—¶é—´æˆ³
                f.write("# æ¨¡å‹ç®¡ç†å™¨æµ‹è¯•ç»“æœ\n\n")
                f.write(f"**æµ‹è¯•æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                f.write("---\n\n")
                
                # å†™å…¥æ‰€æœ‰æ—¥å¿—
                for line in self.lines:
                    # è½¬æ¢æ ¼å¼ä¸º markdown
                    if line.startswith("="*50):
                        f.write("\n---\n\n")
                    elif "æµ‹è¯•" in line and not line.startswith(" "):
                        f.write(f"## {line}\n\n")
                    elif line.startswith("ğŸ“") or line.startswith("ğŸ“‹") or line.startswith("ğŸ–¼ï¸") or line.startswith("ğŸš€"):
                        f.write(f"### {line}\n\n")
                    elif line.startswith("   "):
                        # ç»“æœè¡Œ
                        f.write(f"```\n{line.strip()}\n```\n\n")
                    elif line.startswith("âš ï¸") or line.startswith("âŒ"):
                        f.write(f"> {line}\n\n")
                    elif line.startswith("ğŸ“"):
                        f.write(f"**{line}**\n\n")
                    else:
                        f.write(f"{line}\n\n")
            
            print(f"\nğŸ“„ æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {self.output_file}")
    
    # åˆå§‹åŒ–æ—¥å¿—
    logger = TestLogger()
    log = logger.log
    
    log("ğŸš€ æ¨¡å‹ç®¡ç†å™¨æµ‹è¯•")
    
    # é€šç”¨æµ‹è¯•æ¶ˆæ¯
    simple_messages = [HumanMessage(content="Say hello in one word")]
    format_messages = [HumanMessage(content="ç”Ÿæˆä¸€ä¸ªè™šæ„äººç‰©çš„ä¿¡æ¯ï¼ŒåŒ…å«å§“åã€å¹´é¾„å’Œçˆ±å¥½ã€‚")]
    
    # é€šç”¨å›¾ç‰‡æ¶ˆæ¯ï¼ˆç½‘ç»œ URLï¼‰
    image_messages = [
        HumanMessage(content=[
            {"type": "text", "text": "è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿè¯·ç”¨ä¸­æ–‡ç®€çŸ­æè¿°ã€‚"},
            {
                "type": "image_url",
                "image_url": {
                    "url": "https://q7.itc.cn/q_70/images03/20250219/6c6b4e75e7e6412999a728d67ba7a8d2.jpeg"
                }
            }
        ])
    ]
    
    # é€šç”¨ç»“æ„åŒ–è¾“å‡ºæ ¼å¼
    structured_format = {
        "type": "json_schema",
        "json_schema": {
            "name": "person_info",
            "strict": True,
            "schema": {
                "type": "object",
                "properties": {
                    "name": {"type": "string", "description": "äººç‰©å§“å"},
                    "age": {"type": "integer", "description": "å¹´é¾„"},
                    "hobbies": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "çˆ±å¥½åˆ—è¡¨"
                    }
                },
                "required": ["name", "age", "hobbies"],
                "additionalProperties": False
            }
        }
    }
    
    # JSON æ ¼å¼ï¼ˆå…¼å®¹ä¸æ”¯æŒ json_schema çš„æ¨¡å‹ï¼‰
    json_format = {"type": "json_object"}
    
    def parse_person(content: str):
        """è§£æäººç‰©ä¿¡æ¯ JSON"""
        try:
            person = json_module.loads(content)
            return f"å§“å={person.get('name')}, å¹´é¾„={person.get('age')}, çˆ±å¥½={person.get('hobbies')}"
        except:
            return content
    
    # ============================================
    # 1ï¸âƒ£ OpenAI æµ‹è¯•
    # ============================================
    log("\n" + "="*50)
    log("1ï¸âƒ£ OpenAI æ¨¡å‹æµ‹è¯•")
    log("="*50)
    
    try:
        log("\nğŸ“ åŸºæœ¬é—®ç­”æµ‹è¯•completions (gpt-4o)...")
        resp = completion(model="openai/gpt-4o", messages=simple_messages, response_type="content")
        log(f"   Response: {resp}")

        log("\nğŸ“ åŸºæœ¬é—®ç­”æµ‹è¯•response (gpt-4o)...")
        resp = response(model="openai/gpt-4o", messages=simple_messages, response_type="content")
        log(f"   Response: {resp}")
        
        log("\nğŸ“‹ ç»“æ„åŒ–è¾“å‡ºæµ‹è¯• (gpt-4o)...")
        resp = completion(
            model="openai/gpt-4o", 
            messages=format_messages, 
            response_format=structured_format,
            response_type="content"
        )
        log(f"   Structured: {parse_person(resp)}")
        
        log("\nğŸ–¼ï¸ å›¾ç‰‡ç†è§£æµ‹è¯•-ç½‘ç»œURL (gpt-5)...")
        resp = response(model="openai/gpt-5", messages=image_messages, response_type="raw")
        log(f"   å›¾ç‰‡æè¿°: {resp}")
        
        log("\nğŸš€ Response API æµ‹è¯• (gpt-5)...")
        resp = response(model="openai/gpt-5", messages=simple_messages, response_type="content")
        log(f"   Response: {resp}")
        
    except Exception as e:
        log(f"   âŒ OpenAI æµ‹è¯•å¤±è´¥: {e}")
    
    # ============================================
    # ğŸ–¼ï¸ æœ¬åœ°å›¾ç‰‡æµ‹è¯•ï¼ˆç‹¬ç«‹æµ‹è¯•å—ï¼‰
    # ============================================
    log("\n" + "="*50)
    log("ğŸ–¼ï¸ æœ¬åœ°å›¾ç‰‡ä¸Šä¼ æµ‹è¯•")
    log("="*50)
    
    try:
        # æµ‹è¯•æœ¬åœ°å›¾ç‰‡è·¯å¾„ï¼ˆè¯·æ›¿æ¢ä¸ºå®é™…å­˜åœ¨çš„å›¾ç‰‡è·¯å¾„ï¼‰
        local_image_path = "./test_image/img1.webp"
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        import os
        if os.path.exists(local_image_path):
            log(f"\nğŸ“ æ‰¾åˆ°æœ¬åœ°å›¾ç‰‡: {local_image_path}")
            
            # ç›´æ¥ä½¿ç”¨ HumanMessage æ„é€ æœ¬åœ°å›¾ç‰‡æ¶ˆæ¯
            local_image_msgs = [
                HumanMessage(content=[
                    {"type": "text", "text": "è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿè¯·ç”¨ä¸­æ–‡ç®€çŸ­æè¿°ã€‚"},
                    {"type": "image_url", "image_url": {"url": local_image_path}}
                ])
            ]
            
            log("\nğŸ–¼ï¸ æœ¬åœ°å›¾ç‰‡æµ‹è¯• (gpt-4o - completion)...")
            resp = completion(model="openai/gpt-4o", messages=local_image_msgs, response_type="content")
            log(f"   å›¾ç‰‡æè¿°: {resp}")
            
            log("\nğŸ–¼ï¸ æœ¬åœ°å›¾ç‰‡æµ‹è¯• (gemini-2.5-pro - completion)...")
            resp = completion(model="gemini-2.5-pro", messages=local_image_msgs, response_type="content")
            log(f"   å›¾ç‰‡æè¿°: {resp}")
        else:
            log(f"\nâš ï¸ æœ¬åœ°å›¾ç‰‡ä¸å­˜åœ¨: {local_image_path}")
            log("   è¯·åˆ›å»ºæµ‹è¯•å›¾ç‰‡æˆ–ä¿®æ”¹ local_image_path å˜é‡")
            
    except Exception as e:
        log(f"   âŒ æœ¬åœ°å›¾ç‰‡æµ‹è¯•å¤±è´¥: {e}")
    
    # ============================================
    # 2ï¸âƒ£ Qwen (é€šä¹‰åƒé—®) æµ‹è¯•
    # ============================================
    log("\n" + "="*50)
    log("2ï¸âƒ£ Qwen (é€šä¹‰åƒé—®) æ¨¡å‹æµ‹è¯•")
    log("="*50)
    
    try:
        log("\nğŸ“ åŸºæœ¬é—®ç­”æµ‹è¯• (qwen-plus)...")
        resp = completion(model="qwen-plus", messages=simple_messages, response_type="content")
        log(f"   Response: {resp}")
        
        log("\nğŸ“‹ ç»“æ„åŒ–è¾“å‡ºæµ‹è¯• (qwen-plus)...")
        # Qwen ä½¿ç”¨ json_object æ ¼å¼
        qwen_format_messages = [HumanMessage(content="ç”Ÿæˆä¸€ä¸ªè™šæ„äººç‰©çš„JSONä¿¡æ¯ï¼ŒåŒ…å«name(å§“å)ã€age(å¹´é¾„)å’Œhobbies(çˆ±å¥½æ•°ç»„)å­—æ®µã€‚åªè¾“å‡ºJSONã€‚")]
        resp = completion(
            model="qwen-plus", 
            messages=qwen_format_messages, 
            response_format=json_format,
            response_type="content"
        )
        log(f"   Structured: {parse_person(resp)}")
        
        log("\nğŸ–¼ï¸ å›¾ç‰‡ç†è§£æµ‹è¯• (qwen3-vl-plus)...")
        resp = completion(model="qwen3-vl-plus", messages=image_messages, response_type="content")
        log(f"   å›¾ç‰‡æè¿°: {resp}")
        
    except Exception as e:
        log(f"   âŒ Qwen æµ‹è¯•å¤±è´¥: {e}")
    
    # ============================================
    # 3ï¸âƒ£ DeepSeek æµ‹è¯•
    # ============================================
    log("\n" + "="*50)
    log("3ï¸âƒ£ DeepSeek æ¨¡å‹æµ‹è¯•")
    log("="*50)
    
    try:
        log("\nğŸ“ åŸºæœ¬é—®ç­”æµ‹è¯• (deepseek-v3.2-exp)...")
        resp = completion(model="deepseek-v3.2-exp", messages=simple_messages, response_type="content")
        log(f"   Response: {resp}")
        
        log("\nğŸ“‹ ç»“æ„åŒ–è¾“å‡ºæµ‹è¯• (deepseek-v3.2-exp)...")
        deepseek_format_messages = [HumanMessage(content="ç”Ÿæˆä¸€ä¸ªè™šæ„äººç‰©çš„JSONä¿¡æ¯ï¼ŒåŒ…å«name(å§“å)ã€age(å¹´é¾„)å’Œhobbies(çˆ±å¥½æ•°ç»„)å­—æ®µã€‚åªè¾“å‡ºJSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚")]
        resp = completion(
            model="deepseek-v3.2-exp", 
            messages=deepseek_format_messages, 
            response_format=json_format,
            response_type="content"
        )
        log(f"   Structured: {parse_person(resp)}")
        
    except Exception as e:
        log(f"   âŒ DeepSeek æµ‹è¯•å¤±è´¥: {e}")
    
    # ============================================
    # 4ï¸âƒ£ Claude (Anthropic) æµ‹è¯•
    # ============================================
    log("\n" + "="*50)
    log("4ï¸âƒ£ Claude (Anthropic) æ¨¡å‹æµ‹è¯•")
    log("="*50)
    
    try:
        log("\nğŸ“ åŸºæœ¬é—®ç­”æµ‹è¯• (claude-sonnet-4-5-20250929)...")
        resp = completion(model="claude-sonnet-4-5-20250929", messages=simple_messages, response_type="content")
        log(f"   Response: {resp}")
        
        log("\nğŸ“‹ ç»“æ„åŒ–è¾“å‡ºæµ‹è¯• (claude-sonnet-4-5-20250929)...")
        claude_format_messages = [HumanMessage(content="ç”Ÿæˆä¸€ä¸ªè™šæ„äººç‰©çš„JSONä¿¡æ¯ï¼ŒåŒ…å«name(å§“å)ã€age(å¹´é¾„)å’Œhobbies(çˆ±å¥½æ•°ç»„)å­—æ®µã€‚åªè¾“å‡ºçº¯JSONï¼Œä¸è¦markdownä»£ç å—ã€‚")]
        resp = completion(
            model="claude-sonnet-4-5-20250929", 
            messages=claude_format_messages, 
            response_type="content"
        )
        log(f"   Structured: {parse_person(resp)}")
        
    except Exception as e:
        log(f"   âŒ Claude æµ‹è¯•å¤±è´¥: {e}")
    
    # ============================================
    # 5ï¸âƒ£ Gemini (Google) æµ‹è¯•
    # ============================================
    log("\n" + "="*50)
    log("5ï¸âƒ£ Gemini (Google) æ¨¡å‹æµ‹è¯•")
    log("="*50)
    
    try:
        log("\nğŸ“ åŸºæœ¬é—®ç­”æµ‹è¯• (gemini-2.5-pro)...")
        resp = completion(model="gemini-2.5-pro", messages=simple_messages, response_type="content")
        log(f"   Response: {resp}")
        
        log("\nğŸ“‹ ç»“æ„åŒ–è¾“å‡ºæµ‹è¯• (gemini-2.5-pro)...")
        gemini_format_messages = [HumanMessage(content="ç”Ÿæˆä¸€ä¸ªè™šæ„äººç‰©çš„JSONä¿¡æ¯ï¼ŒåŒ…å«name(å§“å)ã€age(å¹´é¾„)å’Œhobbies(çˆ±å¥½æ•°ç»„)å­—æ®µã€‚åªè¾“å‡ºçº¯JSONã€‚")]
        resp = completion(
            model="gemini-2.5-pro", 
            messages=gemini_format_messages, 
            response_format=json_format,
            response_type="content"
        )
        log(f"   Structured: {parse_person(resp)}")
        
        log("\nğŸ–¼ï¸ å›¾ç‰‡ç†è§£æµ‹è¯• (gemini-2.5-pro)...")
        resp = completion(model="gemini-2.5-pro", messages=image_messages, response_type="content")
        log(f"   å›¾ç‰‡æè¿°: {resp}")
        
    except Exception as e:
        log(f"   âŒ Gemini æµ‹è¯•å¤±è´¥: {e}")
    
    log("\n" + "="*50)
    log("âœ… æµ‹è¯•å®Œæˆ")
    log("="*50)
    
    # ä¿å­˜æµ‹è¯•ç»“æœåˆ° md æ–‡ä»¶
    logger.save()
