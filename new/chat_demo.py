#!/usr/bin/env python
"""
å®é™…èŠå¤©æ¼”ç¤º - çœŸå®è°ƒç”¨ API
å±•ç¤ºå¦‚ä½•ä½¿ç”¨ ModelManager å’Œ MessageManager æ„å»ºçœŸå®çš„èŠå¤©åº”ç”¨
"""

import os
from model_manager import model_manager
from message_manager import MessageManager
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


def _generate_model_descriptions():
    """
    ä» ModelManager åŠ¨æ€ç”Ÿæˆæ¨¡å‹åˆ—è¡¨ï¼ˆåŒ…æ‹¬æ‰€æœ‰å·²æ³¨å†Œçš„æ¨¡å‹ï¼‰
    """
    available_models = {}
    idx = 1
    
    # æ”¶é›†æ‰€æœ‰æ¨¡å‹å’Œå®ƒä»¬çš„æä¾›å•†
    all_models = {}
    
    # 1. ä» known_models è·å–é¢„å®šä¹‰æ¨¡å‹
    for model_name, provider in model_manager.known_models.items():
        all_models[model_name] = provider
    
    # 2. ä» models è·å–é€šè¿‡ register_model æ³¨å†Œçš„æ¨¡å‹ï¼ˆåŒ…æ‹¬è‡ªå®šä¹‰æ¨¡å‹ï¼‰
    for model_name, config in model_manager.models.items():
        if model_name not in all_models:  # é¿å…é‡å¤
            all_models[model_name] = config.provider
    
    # 3. æŒ‰æä¾›å•†åˆ†ç»„
    models_by_provider = {}
    for model_name, provider in all_models.items():
        if provider not in models_by_provider:
            models_by_provider[provider] = []
        models_by_provider[provider].append(model_name)
    
    # 4. æŒ‰æä¾›å•†æ’åºï¼ˆä¼˜å…ˆæ˜¾ç¤ºå¸¸ç”¨æä¾›å•†ï¼Œå…¶ä»–æŒ‰å­—æ¯é¡ºåºï¼‰
    provider_priority = ["openai", "anthropic", "deepseek", "google", "groq", "mistral"]
    sorted_providers = []
    
    # å…ˆæ·»åŠ ä¼˜å…ˆæä¾›å•†ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    for provider in provider_priority:
        if provider in models_by_provider:
            sorted_providers.append(provider)
    
    # å†æ·»åŠ å…¶ä»–æä¾›å•†ï¼ˆæŒ‰å­—æ¯é¡ºåºï¼‰
    other_providers = sorted([p for p in models_by_provider.keys() if p not in provider_priority])
    sorted_providers.extend(other_providers)
    
    # 5. ç”Ÿæˆæ¨¡å‹åˆ—è¡¨
    for provider in sorted_providers:
        # å¯¹æ¯ä¸ªæä¾›å•†çš„æ¨¡å‹æŒ‰å­—æ¯æ’åº
        for model_name in sorted(models_by_provider[provider]):
            description = f"{provider.upper()} - {model_name}"
            available_models[str(idx)] = (model_name, description)
            idx += 1
    
    return available_models


# åŠ¨æ€ç”Ÿæˆå¯ç”¨æ¨¡å‹é…ç½®ï¼ˆä¼šåœ¨æ¯æ¬¡è°ƒç”¨æ—¶åˆ·æ–°ï¼‰
def get_available_models():
    """è·å–å½“å‰æ‰€æœ‰å¯ç”¨æ¨¡å‹ï¼ˆåŒ…æ‹¬æ–°æ³¨å†Œçš„ï¼‰"""
    return _generate_model_descriptions()

# åˆå§‹åŒ–æ—¶ç”Ÿæˆä¸€æ¬¡ï¼ˆå‘åå…¼å®¹ï¼‰
AVAILABLE_MODELS = get_available_models()


def show_models_list(show_provider: bool = True):
    """
    æ˜¾ç¤ºæ‰€æœ‰å¯ç”¨æ¨¡å‹åˆ—è¡¨
    
    Args:
        show_provider: æ˜¯å¦æ˜¾ç¤ºæä¾›å•†ä¿¡æ¯
    """
    # è·å–æœ€æ–°çš„æ¨¡å‹åˆ—è¡¨ï¼ˆåŒ…æ‹¬æ–°æ³¨å†Œçš„ï¼‰
    models = get_available_models()
    
    print("\nğŸ“‹ å¯ç”¨æ¨¡å‹åˆ—è¡¨:")
    print("-" * 70)
    
    if show_provider:
        # æŒ‰æä¾›å•†åˆ†ç»„æ˜¾ç¤º
        current_provider = None
        for key, (model, desc) in models.items():
            provider = model_manager.known_models.get(model, "unknown")
            
            if provider != current_provider:
                if current_provider is not None:
                    print()
                print(f"  ã€{provider.upper()}ã€‘")
                current_provider = provider
            
            print(f"    {key:2}. {desc}")
    else:
        # ç®€å•åˆ—è¡¨
        for key, (model, desc) in models.items():
            print(f"  {key:2}. {desc}")
    
    print("-" * 70)


def select_model(prompt: str = "è¯·é€‰æ‹©æ¨¡å‹", default: str = "1", show_provider: bool = False) -> tuple:
    """
    è®©ç”¨æˆ·é€‰æ‹©æ¨¡å‹
    
    Args:
        prompt: æç¤ºä¿¡æ¯
        default: é»˜è®¤é€‰é¡¹
        show_provider: æ˜¯å¦æ˜¾ç¤ºæä¾›å•†åˆ†ç»„
        
    Returns:
        tuple: (model_name, model_description)
    """
    # è·å–æœ€æ–°çš„æ¨¡å‹åˆ—è¡¨
    models = get_available_models()
    
    print(f"\n{prompt}:")
    
    if show_provider:
        show_models_list(show_provider=True)
    else:
        print("-" * 70)
        for key, (model, desc) in models.items():
            print(f"  {key:2}. {desc}")
        print("-" * 70)
    
    choice = input(f"\nè¯·è¾“å…¥é€‰é¡¹ (1-{len(models)}ï¼Œé»˜è®¤ {default}): ").strip() or default
    
    if choice not in models:
        print(f"âŒ æ— æ•ˆé€‰æ‹©ï¼Œä½¿ç”¨é»˜è®¤é€‰é¡¹ {default}")
        choice = default
    
    model_name, model_desc = models[choice]
    
    # è·å–æ¨¡å‹çš„æä¾›å•†ä¿¡æ¯
    provider = model_manager.known_models.get(model_name, "unknown")
    print(f"âœ… å·²é€‰æ‹©: {model_desc} (æä¾›å•†: {provider})\n")
    
    return model_name, model_desc


# æ–¹å¼ 1: ä½¿ç”¨ requests ç›´æ¥è°ƒç”¨ï¼ˆæœ€åŸºç¡€ï¼‰
def chat_with_requests(model: str, messages: list):
    """ä½¿ç”¨ requests åº“ç›´æ¥è°ƒç”¨ API"""
    import requests
    
    # è·å–æ¨¡å‹é…ç½®
    model_name, provider, api_key, api_base = model_manager.get_llm_provider(model)
    
    print(f"\nğŸ“¡ è°ƒç”¨æ¨¡å‹: {model_name}")
    print(f"   æä¾›å•†: {provider}")
    print(f"   API Base: {api_base}")
    
    # æ„å»ºè¯·æ±‚
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    payload = {
        "model": model_name,
        "messages": messages,
        "temperature": 0.7,
        "stream": False
    }
    
    # å‘é€è¯·æ±‚
    response = requests.post(
        f"{api_base}/chat/completions",
        headers=headers,
        json=payload,
        timeout=30
    )
    
    if response.status_code == 200:
        result = response.json()
        return result["choices"][0]["message"]["content"]
    else:
        raise Exception(f"API è°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")


# æ–¹å¼ 2: ä½¿ç”¨ OpenAI SDKï¼ˆæ¨èï¼‰
def chat_with_openai_sdk(model: str, messages: list):
    """ä½¿ç”¨ OpenAI SDK è°ƒç”¨ APIï¼ˆå…¼å®¹æ‰€æœ‰ OpenAI æ ¼å¼çš„ APIï¼‰"""
    from openai import OpenAI
    
    # è·å–æ¨¡å‹é…ç½®
    model_name, provider, api_key, api_base = model_manager.get_llm_provider(model)
    
    # åˆ›å»ºå®¢æˆ·ç«¯
    client = OpenAI(
        api_key=api_key,
        base_url=api_base
    )
    
    print(f"\nğŸ“¡ è°ƒç”¨æ¨¡å‹: {model_name}")
    print(f"   æä¾›å•†: {provider}")
    print(f"   API Base: {api_base}")
    
    # å‘é€è¯·æ±‚
    response = client.chat.completions.create(
        model=model_name,
        messages=messages,
        temperature=0.7
    )
    
    return response.choices[0].message.content


# æ–¹å¼ 3: ä½¿ç”¨ LiteLLMï¼ˆæœ€çµæ´»ï¼‰
def chat_with_litellm(model: str, messages: list):
    """ä½¿ç”¨ LiteLLM è°ƒç”¨ APIï¼ˆæ”¯æŒæ‰€æœ‰ä¸»æµæ¨¡å‹æä¾›å•†ï¼‰"""
    import litellm
    
    # è·å–æ¨¡å‹é…ç½®
    model_name, provider, api_key, api_base = model_manager.get_llm_provider(model)
    
    print(f"\nğŸ“¡ è°ƒç”¨æ¨¡å‹: {model_name}")
    print(f"   æä¾›å•†: {provider}")
    print(f"   API Base: {api_base}")
    
    # å‘é€è¯·æ±‚
    response = litellm.completion(
        model=f"{provider}/{model_name}",
        messages=messages,
        api_key=api_key,
        api_base=api_base,
        temperature=0.7
    )
    
    return response.choices[0].message.content


# å®Œæ•´çš„èŠå¤©æœºå™¨äººç±»
class ChatBot:
    """å®Œæ•´çš„èŠå¤©æœºå™¨äººå®ç°"""
    
    def __init__(self, model: str, system_prompt: str = None, method: str = "openai"):
        """
        åˆå§‹åŒ–èŠå¤©æœºå™¨äºº
        
        Args:
            model: æ¨¡å‹åç§°ï¼ˆå¦‚ "gpt-4", "groq/llama-3.1-8b" ç­‰ï¼‰
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            method: è°ƒç”¨æ–¹å¼ ("requests", "openai", "litellm")
        """
        self.model = model
        self.method = method
        self.message_manager = MessageManager(
            system_prompt=system_prompt or "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„ AI åŠ©æ‰‹ã€‚"
        )
        
        # éªŒè¯æ¨¡å‹é…ç½®
        try:
            model_name, provider, api_key, api_base = model_manager.get_llm_provider(model)
            print(f"âœ… èŠå¤©æœºå™¨äººå·²åˆå§‹åŒ–")
            print(f"   æ¨¡å‹: {model_name}")
            print(f"   æä¾›å•†: {provider}")
            print(f"   API Base: {api_base}")
            print(f"   è°ƒç”¨æ–¹å¼: {method}")
        except Exception as e:
            print(f"âŒ æ¨¡å‹é…ç½®å¤±è´¥: {e}")
            raise
    
    def chat(self, user_input: str, images: list = None) -> str:
        """
        ä¸æœºå™¨äººå¯¹è¯
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            images: å¯é€‰çš„å›¾åƒ URL åˆ—è¡¨ï¼ˆç”¨äºå¤šæ¨¡æ€å¯¹è¯ï¼‰
            
        Returns:
            åŠ©æ‰‹çš„å›å¤
        """
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        if images:
            self.message_manager.add_multimodal_message(
                role="user",
                text=user_input,
                images=images
            )
        else:
            self.message_manager.add_user_message(user_input)
        
        # è·å–æ¶ˆæ¯å†å²
        messages = self.message_manager.get_messages(format="dict")
        
        # è°ƒç”¨ API
        try:
            if self.method == "requests":
                response = chat_with_requests(self.model, messages)
            elif self.method == "openai":
                response = chat_with_openai_sdk(self.model, messages)
            elif self.method == "litellm":
                response = chat_with_litellm(self.model, messages)
            else:
                raise ValueError(f"æœªçŸ¥çš„è°ƒç”¨æ–¹å¼: {self.method}")
            
            # æ£€æŸ¥å“åº”æ˜¯å¦ä¸ºç©º
            if not response or not response.strip():
                error_msg = "âŒ API è¿”å›ç©ºå“åº”"
                print(error_msg)
                # ç§»é™¤åˆšæ‰æ·»åŠ çš„ç”¨æˆ·æ¶ˆæ¯
                self.message_manager.pop_last_message()
                return error_msg
            
            # æ·»åŠ åŠ©æ‰‹å›å¤
            self.message_manager.add_assistant_message(response)
            
            return response
            
        except Exception as e:
            error_msg = f"âŒ API è°ƒç”¨å¤±è´¥: {str(e)}"
            print(error_msg)
            # ç§»é™¤åˆšæ‰æ·»åŠ çš„ç”¨æˆ·æ¶ˆæ¯
            self.message_manager.pop_last_message()
            return error_msg
    
    def chat_stream(self, user_input: str):
        """æµå¼å¯¹è¯ï¼ˆå®æ—¶è¿”å›ï¼‰"""
        from openai import OpenAI
        
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        self.message_manager.add_user_message(user_input)
        messages = self.message_manager.get_messages(format="dict")
        
        # è·å–æ¨¡å‹é…ç½®
        model_name, provider, api_key, api_base = model_manager.get_llm_provider(self.model)
        
        # åˆ›å»ºå®¢æˆ·ç«¯
        client = OpenAI(api_key=api_key, base_url=api_base)
        
        print(f"\nğŸ¤– åŠ©æ‰‹: ", end="", flush=True)
        
        try:
            # æµå¼è¯·æ±‚
            full_response = ""
            stream = client.chat.completions.create(
                model=model_name,
                messages=messages,
                stream=True,
                temperature=0.7
            )
            
            for chunk in stream:
                # æ›´å¥å£®çš„é”™è¯¯å¤„ç† - æ£€æŸ¥ chunk ç»“æ„
                if hasattr(chunk, 'choices') and len(chunk.choices) > 0:
                    delta = chunk.choices[0].delta
                    if hasattr(delta, 'content') and delta.content:
                        content = delta.content
                        print(content, end="", flush=True)
                        full_response += content
            
            print()  # æ¢è¡Œ
            
            # æ£€æŸ¥å“åº”æ˜¯å¦ä¸ºç©º
            if not full_response.strip():
                error_msg = "ï¼ˆAPI è¿”å›ç©ºå“åº”ï¼‰"
                print(error_msg)
                # ç§»é™¤åˆšæ‰æ·»åŠ çš„ç”¨æˆ·æ¶ˆæ¯ï¼Œå› ä¸ºå¯¹è¯å¤±è´¥äº†
                self.message_manager.pop_last_message()
                return error_msg
            
            # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°å†å²
            self.message_manager.add_assistant_message(full_response)
            
            return full_response
            
        except Exception as e:
            error_msg = f"\nâŒ API è°ƒç”¨å¤±è´¥: {str(e)}"
            print(error_msg)
            # ç§»é™¤åˆšæ‰æ·»åŠ çš„ç”¨æˆ·æ¶ˆæ¯ï¼Œå› ä¸ºå¯¹è¯å¤±è´¥äº†
            self.message_manager.pop_last_message()
            return error_msg
    
    def print_history(self):
        """æ‰“å°å¯¹è¯å†å²"""
        self.message_manager.print_history()
    
    def clear_history(self):
        """æ¸…ç©ºå¯¹è¯å†å²"""
        self.message_manager.clear_history()
    
    def export_chat(self, filepath: str):
        """å¯¼å‡ºå¯¹è¯å†å²"""
        self.message_manager.export_history(filepath)


# ç¤ºä¾‹ 1: åŸºç¡€å¯¹è¯
def demo_basic_chat():
    print("=" * 70)
    print("ğŸ“± ç¤ºä¾‹ 1: åŸºç¡€å¯¹è¯")
    print("=" * 70)
    
    # è®©ç”¨æˆ·é€‰æ‹©æ¨¡å‹
    selected_model, _ = select_model("é€‰æ‹©è¦ä½¿ç”¨çš„æ¨¡å‹", default="1")
    
    # åˆ›å»ºèŠå¤©æœºå™¨äººï¼ˆä½¿ç”¨ä½ çš„ä»£ç† APIï¼‰
    bot = ChatBot(
        model=selected_model,
        system_prompt="ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„ AI åŠ©æ‰‹ï¼Œè¯·ç”¨ç®€æ´çš„è¯­è¨€å›ç­”ã€‚",
        method="openai"  # ä½¿ç”¨ OpenAI SDK
    )
    
    # è¿›è¡Œå¯¹è¯
    print("\nğŸ‘¤ ç”¨æˆ·: ä½ å¥½ï¼")
    response = bot.chat("ä½ å¥½ï¼")
    print(f"ğŸ¤– åŠ©æ‰‹: {response}")
    
    print("\nğŸ‘¤ ç”¨æˆ·: ç”¨ Python å†™ä¸€ä¸ª Hello World")
    response = bot.chat("ç”¨ Python å†™ä¸€ä¸ª Hello World")
    print(f"ğŸ¤– åŠ©æ‰‹: {response}")
    
    # æŸ¥çœ‹å†å²
    print("\n" + "=" * 70)
    bot.print_history()


# ç¤ºä¾‹ 2: æµå¼å¯¹è¯
def demo_stream_chat():
    print("\n" + "=" * 70)
    print("âš¡ ç¤ºä¾‹ 2: æµå¼å¯¹è¯ï¼ˆå®æ—¶è¿”å›ï¼‰")
    print("=" * 70)
    
    # è®©ç”¨æˆ·é€‰æ‹©æ¨¡å‹
    selected_model, _ = select_model("é€‰æ‹©è¦ä½¿ç”¨çš„æ¨¡å‹", default="1")
    
    bot = ChatBot(
        model=selected_model,
        system_prompt="ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„ç¼–ç¨‹åŠ©æ‰‹ã€‚",
        method="openai"
    )
    
    print("\nğŸ‘¤ ç”¨æˆ·: è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯è£…é¥°å™¨")
    bot.chat_stream("è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯è£…é¥°å™¨")


# ç¤ºä¾‹ 3: å¤šæ¨¡æ€å¯¹è¯
def demo_multimodal_chat():
    print("\n" + "=" * 70)
    print("ğŸ¨ ç¤ºä¾‹ 3: å¤šæ¨¡æ€å¯¹è¯ï¼ˆæ–‡æœ¬ + å›¾åƒï¼‰")
    print("=" * 70)
    
    bot = ChatBot(
        model="gpt-4o",  # éœ€è¦æ”¯æŒè§†è§‰çš„æ¨¡å‹
        system_prompt="ä½ æ˜¯ä¸€ä¸ªå¯ä»¥ç†è§£å›¾åƒçš„ AI åŠ©æ‰‹ã€‚",
        method="openai"
    )
    
    print("\nğŸ‘¤ ç”¨æˆ·: [å‘é€å›¾ç‰‡] è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿ")
    response = bot.chat(
        "è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿ",
        images=["https://example.com/image.jpg"]
    )
    print(f"ğŸ¤– åŠ©æ‰‹: {response}")


# ç¤ºä¾‹ 4: æ¨¡å‹å¯¹æ¯”
def demo_model_comparison():
    """åŒæ—¶å‘å¤šä¸ªæ¨¡å‹æé—®ï¼Œå¯¹æ¯”å›ç­”"""
    print("\n" + "=" * 70)
    print("âš–ï¸  ç¤ºä¾‹ 4: æ¨¡å‹å¯¹æ¯”")
    print("=" * 70)
    
    # è·å–æœ€æ–°æ¨¡å‹åˆ—è¡¨
    models = get_available_models()
    
    print("\nğŸ“ é€‰æ‹©è¦å¯¹æ¯”çš„æ¨¡å‹ï¼ˆè‡³å°‘é€‰æ‹© 2 ä¸ªï¼Œæœ€å¤š 4 ä¸ªï¼‰")
    print("   è¾“å…¥æ¨¡å‹ç¼–å·ï¼Œç”¨ç©ºæ ¼æˆ–é€—å·åˆ†éš”ï¼Œä¾‹å¦‚: 1 5 6")
    print()
    
    # æ˜¾ç¤ºæ¨¡å‹åˆ—è¡¨
    for key, (model, desc) in models.items():
        print(f"  {key:2}. {desc}")
    
    # è·å–ç”¨æˆ·é€‰æ‹©
    choices = input("\nè¯·è¾“å…¥æ¨¡å‹ç¼–å·: ").strip()
    choices = choices.replace(",", " ").split()
    
    if len(choices) < 2:
        print("âŒ è‡³å°‘éœ€è¦é€‰æ‹© 2 ä¸ªæ¨¡å‹")
        return
    
    if len(choices) > 4:
        print("âš ï¸  æœ€å¤šæ”¯æŒ 4 ä¸ªæ¨¡å‹ï¼Œåªä½¿ç”¨å‰ 4 ä¸ª")
        choices = choices[:4]
    
    # åˆ›å»ºå¤šä¸ªèŠå¤©æœºå™¨äºº
    bots = []
    for choice in choices:
        if choice in models:
            model_name, model_desc = models[choice]
            try:
                bot = ChatBot(
                    model=model_name,
                    system_prompt="ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„ AI åŠ©æ‰‹ï¼Œè¯·ç”¨ç®€æ´çš„è¯­è¨€å›ç­”ã€‚",
                    method="openai"
                )
                bots.append((model_name, model_desc, bot))
                print(f"âœ… å·²åŠ è½½: {model_desc}")
            except Exception as e:
                print(f"âŒ åŠ è½½å¤±è´¥ {model_desc}: {e}")
    
    if len(bots) < 2:
        print("âŒ å¯ç”¨æ¨¡å‹ä¸è¶³ 2 ä¸ª")
        return
    
    print(f"\nâœ… å·²å‡†å¤‡ {len(bots)} ä¸ªæ¨¡å‹è¿›è¡Œå¯¹æ¯”\n")
    
    # è·å–ç”¨æˆ·é—®é¢˜
    question = input("è¯·è¾“å…¥ä½ çš„é—®é¢˜: ").strip()
    
    if not question:
        print("âŒ é—®é¢˜ä¸èƒ½ä¸ºç©º")
        return
    
    print("\n" + "=" * 70)
    print(f"ğŸ“¢ é—®é¢˜: {question}")
    print("=" * 70)
    
    # ä¾æ¬¡è°ƒç”¨æ¯ä¸ªæ¨¡å‹
    for i, (model_name, model_desc, bot) in enumerate(bots, 1):
        print(f"\nã€æ¨¡å‹ {i}: {model_desc}ã€‘")
        print("-" * 70)
        
        try:
            response = bot.chat(question)
            print(response)
        except Exception as e:
            print(f"âŒ è°ƒç”¨å¤±è´¥: {e}")
        
        print("-" * 70)
    
    print("\nâœ… å¯¹æ¯”å®Œæˆï¼")


# ç¤ºä¾‹ 5: äº¤äº’å¼èŠå¤©
def interactive_chat():
    print("\n" + "=" * 70)
    print("ğŸ’¬ äº¤äº’å¼èŠå¤© (è¾“å…¥ 'quit' é€€å‡º)")
    print("=" * 70)
    
    # è®©ç”¨æˆ·é€‰æ‹©æ¨¡å‹
    selected_model, model_desc = select_model("å¯ç”¨æ¨¡å‹åˆ—è¡¨", default="1")
    
    # åˆ›å»ºèŠå¤©æœºå™¨äºº
    bot = ChatBot(
        model=selected_model,
        system_prompt="ä½ æ˜¯ä¸€ä¸ªå‹å¥½ã€æœ‰å¸®åŠ©çš„ AI åŠ©æ‰‹ã€‚",
        method="openai"
    )
    
    print("\nâœ… èŠå¤©æœºå™¨äººå·²å‡†å¤‡å°±ç»ªï¼å¼€å§‹å¯¹è¯å§...")
    print("ğŸ’¡ æç¤º: è¾“å…¥ /help æŸ¥çœ‹å¯ç”¨å‘½ä»¤\n")
    
    while True:
        try:
            # è·å–ç”¨æˆ·è¾“å…¥
            user_input = input("ğŸ‘¤ ä½ : ").strip()
            
            if not user_input:
                continue
            
            if user_input.lower() in ['quit', 'exit', 'é€€å‡º', 'q']:
                print("\nğŸ‘‹ å†è§ï¼")
                break
            
            # ç‰¹æ®Šå‘½ä»¤
            if user_input == "/help":
                print("\nğŸ“š å¯ç”¨å‘½ä»¤:")
                print("-" * 70)
                print("  /help          - æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯")
                print("  /models        - æ˜¾ç¤ºæ‰€æœ‰å¯ç”¨æ¨¡å‹ï¼ˆæŒ‰æä¾›å•†åˆ†ç»„ï¼‰")
                print("  /switch        - åˆ‡æ¢æ¨¡å‹")
                print("  /current       - æ˜¾ç¤ºå½“å‰æ¨¡å‹ä¿¡æ¯")
                print("  /stats         - æ˜¾ç¤ºæ¨¡å‹ç»Ÿè®¡ä¿¡æ¯")
                print("  /history       - æ˜¾ç¤ºå¯¹è¯å†å²")
                print("  /clear         - æ¸…ç©ºå¯¹è¯å†å²")
                print("  /export <file> - å¯¼å‡ºå¯¹è¯å†å²åˆ°æ–‡ä»¶")
                print("  quit/exit/q    - é€€å‡ºç¨‹åº")
                print("-" * 70)
                
                # è·å–æœ€æ–°æ¨¡å‹æ•°é‡
                current_models = get_available_models()
                print(f"\nğŸ’¡ å½“å‰å…±æ”¯æŒ {len(current_models)} ä¸ªæ¨¡å‹")
                continue
            
            if user_input == "/models":
                # è·å–æœ€æ–°æ¨¡å‹åˆ—è¡¨
                current_models = get_available_models()
                
                print("\nğŸ“‹ å¯ç”¨æ¨¡å‹åˆ—è¡¨:")
                print("-" * 70)
                
                # æŒ‰æä¾›å•†åˆ†ç»„æ˜¾ç¤º
                current_provider = None
                for key, (model, desc) in current_models.items():
                    provider = model_manager.known_models.get(model, "unknown")
                    
                    if provider != current_provider:
                        if current_provider is not None:
                            print()
                        print(f"  ã€{provider.upper()}ã€‘")
                        current_provider = provider
                    
                    current = " â† å½“å‰ä½¿ç”¨" if model == selected_model else ""
                    print(f"    {key:2}. {desc}{current}")
                
                print("-" * 70)
                print(f"\nğŸ’¡ æç¤º: è¾“å…¥ /switch å¯ä»¥åˆ‡æ¢æ¨¡å‹")
                continue
            
            if user_input == "/switch":
                # è·å–æœ€æ–°æ¨¡å‹åˆ—è¡¨
                current_models = get_available_models()
                
                print("\nğŸ”„ åˆ‡æ¢æ¨¡å‹:")
                print("-" * 70)
                
                # æŒ‰æä¾›å•†åˆ†ç»„æ˜¾ç¤º
                current_provider = None
                for key, (model, desc) in current_models.items():
                    provider = model_manager.known_models.get(model, "unknown")
                    
                    if provider != current_provider:
                        if current_provider is not None:
                            print()
                        print(f"  ã€{provider.upper()}ã€‘")
                        current_provider = provider
                    
                    current = " â† å½“å‰ä½¿ç”¨" if model == selected_model else ""
                    print(f"    {key:2}. {desc}{current}")
                
                print("-" * 70)
                
                new_choice = input(f"\nè¯·é€‰æ‹©æ–°æ¨¡å‹ (1-{len(current_models)}): ").strip()
                
                if new_choice in current_models:
                    selected_model, model_desc = current_models[new_choice]
                    provider = model_manager.known_models.get(selected_model, "unknown")
                    print(f"\nğŸ”„ æ­£åœ¨åˆ‡æ¢åˆ°: {model_desc} (æä¾›å•†: {provider})")
                    
                    # åˆ›å»ºæ–°çš„èŠå¤©æœºå™¨äººï¼ˆä¿ç•™å†å²ï¼‰
                    old_messages = bot.message_manager.messages
                    bot = ChatBot(
                        model=selected_model,
                        system_prompt="ä½ æ˜¯ä¸€ä¸ªå‹å¥½ã€æœ‰å¸®åŠ©çš„ AI åŠ©æ‰‹ã€‚",
                        method="openai"
                    )
                    bot.message_manager.messages = old_messages
                    print("âœ… æ¨¡å‹åˆ‡æ¢æˆåŠŸï¼\n")
                else:
                    print("âŒ æ— æ•ˆé€‰æ‹©\n")
                continue
            
            if user_input == "/stats":
                # è·å–æœ€æ–°æ¨¡å‹åˆ—è¡¨
                current_models = get_available_models()
                
                print("\nğŸ“Š æ¨¡å‹ç»Ÿè®¡ä¿¡æ¯:")
                print("-" * 70)
                
                # ç»Ÿè®¡å„æä¾›å•†çš„æ¨¡å‹æ•°é‡
                provider_counts = {}
                for model in model_manager.known_models.values():
                    provider_counts[model] = provider_counts.get(model, 0) + 1
                
                print(f"  æ€»æ¨¡å‹æ•°: {len(current_models)}")
                print(f"  æ”¯æŒçš„æä¾›å•†æ•°: {len(provider_counts)}")
                print()
                print("  å„æä¾›å•†æ¨¡å‹æ•°:")
                for provider, count in sorted(provider_counts.items()):
                    print(f"    â€¢ {provider.title()}: {count} ä¸ª")
                
                print("-" * 70 + "\n")
                continue
            
            if user_input == "/current":
                try:
                    model_name, provider, api_key, api_base = model_manager.get_llm_provider(selected_model)
                    print(f"\nğŸ“Š å½“å‰æ¨¡å‹ä¿¡æ¯:")
                    print("-" * 70)
                    print(f"  æ¨¡å‹åç§°: {model_name}")
                    print(f"  æä¾›å•†: {provider}")
                    print(f"  API Base: {api_base}")
                    print(f"  API Key: {'å·²è®¾ç½® âœ…' if api_key else 'æœªè®¾ç½® âŒ'}")
                    print()
                    print(f"  å¯¹è¯ç»Ÿè®¡:")
                    print(f"    â€¢ æ¶ˆæ¯æ€»æ•°: {len(bot.message_manager.messages)}")
                    print(f"    â€¢ Token ä¼°ç®—: ~{bot.message_manager.count_tokens_estimate()}")
                    msg_stats = bot.message_manager.count_messages()
                    print(f"    â€¢ ç”¨æˆ·æ¶ˆæ¯: {msg_stats.get('user', 0)}")
                    print(f"    â€¢ åŠ©æ‰‹æ¶ˆæ¯: {msg_stats.get('assistant', 0)}")
                    print("-" * 70 + "\n")
                except Exception as e:
                    print(f"âŒ è·å–æ¨¡å‹ä¿¡æ¯å¤±è´¥: {e}\n")
                continue
            
            if user_input == "/history":
                bot.print_history()
                continue
            
            if user_input == "/clear":
                bot.clear_history()
                print("âœ… å†å²å·²æ¸…ç©º\n")
                continue
            
            if user_input.startswith("/export "):
                filepath = user_input.split(" ", 1)[1]
                bot.export_chat(filepath)
                continue
            
            # æ­£å¸¸å¯¹è¯
            # ä½¿ç”¨æµå¼è¾“å‡º
            bot.chat_stream(user_input)
            
            print()  # ç©ºè¡Œ
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {e}\n")


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "ğŸ¯ " * 35)
    print("   å®é™…èŠå¤©æ¼”ç¤º - ä½¿ç”¨çœŸå® API")
    print("ğŸ¯ " * 35 + "\n")
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    if not os.getenv("API_KEY"):
        print("âš ï¸  è­¦å‘Š: æœªæ£€æµ‹åˆ° API_KEY ç¯å¢ƒå˜é‡")
        print("   è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®:")
        print("   BASE_URL=https://your-proxy-api.com/v1")
        print("   API_KEY=your-api-key-here")
        print()
        return
    
    print("âœ… ç¯å¢ƒé…ç½®å·²åŠ è½½")
    print(f"   BASE_URL: {os.getenv('BASE_URL', 'æœªè®¾ç½®')}")
    print(f"   API_KEY: {'å·²è®¾ç½®' if os.getenv('API_KEY') else 'æœªè®¾ç½®'}")
    print()
    
    # é€‰æ‹©è¿è¡Œæ¨¡å¼
    print("è¯·é€‰æ‹©è¿è¡Œæ¨¡å¼:")
    print("  1. åŸºç¡€å¯¹è¯æ¼”ç¤º")
    print("  2. æµå¼å¯¹è¯æ¼”ç¤º")
    print("  3. å¤šæ¨¡æ€å¯¹è¯æ¼”ç¤ºï¼ˆéœ€è¦æ”¯æŒè§†è§‰çš„æ¨¡å‹ï¼‰")
    print("  4. æ¨¡å‹å¯¹æ¯”ï¼ˆåŒæ—¶å‘å¤šä¸ªæ¨¡å‹æé—®ï¼‰")
    print("  5. äº¤äº’å¼èŠå¤©ï¼ˆæ¨èï¼‰â­")
    print("  6. è¿è¡Œæ‰€æœ‰æ¼”ç¤º")
    
    try:
        choice = input("\nè¯·è¾“å…¥é€‰é¡¹ (1-6ï¼Œé»˜è®¤ 5): ").strip() or "5"
        
        if choice == "1":
            demo_basic_chat()
        elif choice == "2":
            demo_stream_chat()
        elif choice == "3":
            demo_multimodal_chat()
        elif choice == "4":
            demo_model_comparison()
        elif choice == "5":
            interactive_chat()
        elif choice == "6":
            demo_basic_chat()
            demo_stream_chat()
            demo_model_comparison()
            # demo_multimodal_chat()  # éœ€è¦æ”¯æŒè§†è§‰çš„æ¨¡å‹
        else:
            print("âŒ æ— æ•ˆé€‰é¡¹")
            
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ç¨‹åºå·²é€€å‡º")


if __name__ == "__main__":
    main()

